{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Deep Learning Cheatsheet FS24\n",
   "id": "483fe10a0f91446e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Code snippets\n",
   "id": "1873f97552555977"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Imports",
   "id": "4834e20a66eb52c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "import copy\n",
    "import zipfile\n",
    "import csv\n",
    "import io\n",
    "import torch\n",
    "import csv\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import torchvision.models\n",
    "import torchvision.transforms as tfs\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "d8605fa5a8c1ca12"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Device",
   "id": "b893fe746620a7ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# normal config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# apple silicon chip\n",
    "mps_device = torch.device(\"mps\")"
   ],
   "id": "b70a2c3190e0c9cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Tasks",
   "id": "d89c745cd641e0e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Data Generation\n",
    "\n",
    "Datasets: \n",
    "\n",
    "1. $X_1: t = \\sin(3x)$ for $x\\in[-1,1]$\n",
    "2. $X_2: t = e^{-4x^2}$ for $x\\in[-1,1]$\n",
    "3. $X_3: t = x^5 + 3x^4 - 6x^3 -12x^2 + 5x + 129$ for $x\\in[-4,2.5]$\n",
    "\n",
    "Generate dataset $X_1$, for $N=60$ samples randomly drawn from range $x\\in[-1,1]$. \\\n",
    "Generate data $X_2$ for $N=50$ samples randomly drawn from range $x\\in[-1,1]$.  \\\n",
    "Generate dataset $X_3$ for $N=200$ samples randomly drawn from range $x\\in[-4,2.5]$. \\\n",
    "Implement all three datasets as lists of tuples: $\\{(\\vec x^{[n]}, t^{[n]})\\mid 1\\leq n\\leq N\\}$. \\"
   ],
   "id": "2bac7f65208e28f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1D list\n",
    "np.random.uniform(low=-1, high=1, size=5 + 1)\n",
    "# 2D list\n",
    "np.random.uniform(low=-1, high=1, size=(5, 10))\n",
    "\n",
    "# special datasets\n",
    "X1 = [(np.array([1, x]), np.sin(3 * x)) for x in np.random.uniform(low=-1, high=1, size=60)]\n",
    "X2 = [(np.array([1, x]), np.exp(-4 * x ** 2)) for x in np.random.uniform(low=-1, high=1, size=50)]\n",
    "X3 = [(np.array([1, x]), x ** 5 + 3 * x ** 4 - 6 * x ** 3 - 12 * x ** 2 + 5 * x + 129) for x in\n",
    "      np.random.uniform(low=-4, high=2.5, size=200)]"
   ],
   "id": "843809aae7273e04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Read Data Examples",
   "id": "eab8c37021579517"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T10:19:57.828313Z",
     "start_time": "2024-06-11T10:19:57.821422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def data_from_zip_as_np_array(course=\"por\"):\n",
    "    # download data file from URL\n",
    "    dataset_zip_file = \"student.zip\"\n",
    "    if not os.path.exists(dataset_zip_file):\n",
    "        import urllib.request\n",
    "        urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00320/student.zip\",\n",
    "                                   dataset_zip_file)\n",
    "        print(\"Downloaded datafile\", dataset_zip_file)\n",
    "    # collect inputs\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    # some default values: yes=1, no=-1\n",
    "    yn = {\"yes\": 1., \"no\": -1.}\n",
    "    # read through dataset (without actually unzipping to a file):\n",
    "    # ... open zip file\n",
    "    zip = zipfile.ZipFile(dataset_zip_file)\n",
    "    # ... open data file inside of zip file and convert bytes to text\n",
    "    datafile = io.TextIOWrapper(zip.open(os.path.join(F\"student-{course}.csv\"), 'r'))\n",
    "    # ... read through the lines via CSV reader, using the correct delimiter\n",
    "    reader = csv.reader(datafile, delimiter=\";\")\n",
    "    # ... skip header line\n",
    "    next(reader)\n",
    "    for splits in reader:\n",
    "        # read input values\n",
    "        inputs.append([\n",
    "            1.,  #### BIAS ####\n",
    "            {\"GP\": 1., \"MS\": -1.}[splits[0]],  # school\n",
    "            float(splits[29]),  # absences\n",
    "        ])\n",
    "        # read targets values\n",
    "        targets.append([\n",
    "            float(splits[32]),  # grade for tertiary school\n",
    "        ])\n",
    "    print(F\"Loaded dataset with {len(targets)} samples\")\n",
    "    return np.array(inputs).transpose(), np.array(targets).transpose()\n",
    "\n",
    "\n",
    "def dataset_from_file(dataset_file=\"winequality-red.csv\", delimiter=\";\"):\n",
    "    # read dataset\n",
    "    with open(dataset_file, 'r') as f:\n",
    "        df = pd.read_csv(filepath_or_buffer=f, delimiter=delimiter, header=0)\n",
    "    # convert to torch.tensor\n",
    "    data = torch.tensor(df.values)\n",
    "    # get the input (data samples) without the target information\n",
    "    X = data[:, :-1].float()\n",
    "    if dataset_file == \"winequality-red.csv\":\n",
    "        # target is in the last column and needs to be converted to long\n",
    "        T = data[:, -1].long()\n",
    "        T = torch.sub(T, 3)\n",
    "    else:\n",
    "        # target is in the last column and needs to be of type float\n",
    "        T = data[:, -1].reshape(-1, 1).float()\n",
    "    return X, T\n",
    "\n",
    "\n",
    "# intialize data\n",
    "X, T = data_from_zip_as_np_array(\"my_dataset\")"
   ],
   "id": "2e7ec7e7f1fbc2d2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Data initialization examples",
   "id": "37ef76057af1e5c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# data initialization\n",
    "K = 15\n",
    "D = len(X)\n",
    "O = 3\n",
    "# Weight initialization Xavier method\n",
    "W1 = np.random.uniform(low=-1 / np.sqrt(D), high=1 / np.sqrt(D), size=(K + 1, D))\n",
    "W2 = np.random.uniform(low=-1 / np.sqrt(K), high=1 / np.sqrt(K), size=(O, K + 1))\n",
    "Theta = [W1, W2]"
   ],
   "id": "2c120f417ffa4ee3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Data split",
   "id": "bc8acf0934958bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def split_training_data(X, T, train_percentage=0.8, shuffle=True):\n",
    "    if shuffle:\n",
    "        # Combine X and T along axis 1\n",
    "        combined_data = np.concatenate((X, T), axis=1)\n",
    "        # Shuffle the combined data along axis 0\n",
    "        np.random.shuffle(combined_data)\n",
    "        # Split X and T again after shuffling\n",
    "        X = combined_data[:, :X.shape[1]]\n",
    "        T = combined_data[:, X.shape[1]:]\n",
    "\n",
    "    # split into 80/20 training/validation\n",
    "    training_number = int(X.shape[0] * train_percentage)\n",
    "    validation_numer = (X.shape[0] - training_number) * -1\n",
    "    X_train = X[:training_number]\n",
    "    T_train = T[:training_number]\n",
    "    X_val = X[validation_numer:]\n",
    "    T_val = T[validation_numer:]\n",
    "\n",
    "    return X_train, T_train, X_val, T_val\n",
    "\n",
    "\n",
    "X_train, T_train, X_val, T_val = split_training_data(X=X, T=T)"
   ],
   "id": "c25e10ab2ccc47bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Torch Datasets",
   "id": "a4118b4e55f2786c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Fashion MNIST dataset\n",
    "def f_mnist_datasets(transform):\n",
    "    trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    # returns PIL.Image.Image without transorm\n",
    "    return trainset, testset\n",
    "\n",
    "\n",
    "trainset, testset = f_mnist_datasets(transform=None)"
   ],
   "id": "e582353d974e55bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Data Loaders\n",
    "Data loaders simplify interaction with data. Shuffle data, create batches, etc. A bridge between dataset and model"
   ],
   "id": "2a5b1461fa7d0695"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "B = 512\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=B, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=B, shuffle=False)"
   ],
   "id": "3ff46c5735edca3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Data transform\n",
    "Convert images to tensors. First resize, then crop, then convert to tensor and normalize values"
   ],
   "id": "3214f2defffcb2cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "imagenet_transform = tfs.Compose([\n",
    "    tfs.Resize(256),\n",
    "    tfs.CenterCrop(224),\n",
    "    tfs.ToTensor(),\n",
    "    tfs.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "id": "69f3f7041f14c40e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Dataset loader - ImageFolder",
   "id": "19f9297b92e3e082"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dir = './intel-image-classification/seg_train/seg_train/'\n",
    "test_dir = './intel-image-classification/seg_test/seg_test/'\n",
    "\n",
    "trainset = ImageFolder(\n",
    "    root=train_dir,\n",
    "    transform=imagenet_transform\n",
    ")\n",
    "\n",
    "testset = ImageFolder(\n",
    "    root=test_dir,\n",
    "    transform=imagenet_transform\n",
    ")"
   ],
   "id": "ca119a53dc5a1b4f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Normalize",
   "id": "89ea684ca451f3da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# get min and max values\n",
    "min_val = np.max(X, axis=1)\n",
    "max_val = np.min(X, axis=1)\n",
    "\n",
    "# assure to handle x_0 correctly\n",
    "min_val[0] = 0\n",
    "max_val[0] = 1\n",
    "\n",
    "\n",
    "def normalize(x, min_val, max_val):\n",
    "    # normalize the given data with the given minimum and maximum values\n",
    "    return np.transpose((x.transpose() - min_val) / (max_val - min_val))\n",
    "\n",
    "\n",
    "# Normalize our dataset\n",
    "X = normalize(X, min_val, max_val)"
   ],
   "id": "2f455a0ddaa0ce3c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Standardize",
   "id": "2fd5446c98f07c1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def standardize(X_train, X_val):\n",
    "    # compute statistics\n",
    "    mean = torch.mean(X_train, dim=0)\n",
    "    std = torch.std(X_train, dim=0)\n",
    "\n",
    "    # standardize both X_train and X_val\n",
    "    X_train = (X_train - mean) / std\n",
    "    X_val = (X_val - mean) / std\n",
    "    return X_train, X_val\n",
    "\n",
    "\n",
    "X_train, X_val = standardize(X_train=X_train, X_val=X_val)"
   ],
   "id": "f696e414ad7b5559"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Accuracy\n",
    "Accuracy check for categorical or binary classification"
   ],
   "id": "3a43e8f857015b81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def accuracy(Z, T):\n",
    "    # check if we have binary or categorical classification\n",
    "    if len(T.shape) == 2:\n",
    "        # binary classification\n",
    "        y = (Z >= 0).float()\n",
    "        return torch.mean((y == T).float())\n",
    "    else:\n",
    "        # categorical classification\n",
    "        y = torch.argmax(Z, dim=1)\n",
    "        return torch.mean((y == T).float())"
   ],
   "id": "722a14c8ed44edf2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Network implementations",
   "id": "bce3c790cf6714ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Activation functions",
   "id": "a828fb435033d682"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def logistic(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ],
   "id": "6027a4489d3f319c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Loss Functions",
   "id": "829acabb1995aa27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Squared Loss\n",
    "$\\mathcal J^{L_2} = \\frac1B \\|\\mathbf Y - \\mathbf T\\|_F^2$ for given network outputs $\\mathbf Y$ and target values $\\mathbf T$."
   ],
   "id": "93c10704b6e879e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def loss(Y, T):\n",
    "    return (1 / T.shape[1]) * np.linalg.norm(Y - T, \"fro\") ** 2"
   ],
   "id": "96e5fcb40ac81a1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Batch creation",
   "id": "4a3e1b774e56f07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 5,
   "source": [
    "# used in enumerate -> yield\n",
    "def batch_with_shuffle(X, T, batch_size=16):\n",
    "    num_of_samples = X.shape[1]\n",
    "    shuffle_idx = np.random.permutation(num_of_samples)\n",
    "    i = 0\n",
    "    new_epoch = True\n",
    "    while True:\n",
    "        # shuffle dataset in each epoch   \n",
    "        if (i + batch_size) >= X.shape[1]:\n",
    "            shuffle_idx = np.random.permutation(X.shape[1])\n",
    "            i = 0\n",
    "            new_epoch = True\n",
    "        # yield the batch\n",
    "        yield X[:, shuffle_idx[i:i + batch_size]], T[:, shuffle_idx[i:i + batch_size]], new_epoch\n",
    "        new_epoch = False\n",
    "        i += batch_size"
   ],
   "id": "9a415197e18dc2bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Network examples",
   "id": "db36fd6cbb896f06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 4,
   "source": [
    "# Network for a given input vector and parameters Theta\n",
    "def simple_network(x, Theta):\n",
    "    W1, w2 = Theta\n",
    "    # linear combination for hidden layer\n",
    "    a_ = np.dot(W1, x)\n",
    "    # activation function\n",
    "    h_ = logistic(a_)\n",
    "    # adding bias\n",
    "    h = np.insert(h_, 0, 1)\n",
    "    y = np.dot(w2, h)\n",
    "    return y, h\n",
    "\n",
    "\n",
    "# Multi-target network -> output matrix Y, hidden unit output H\n",
    "def multi_target_network(X, Theta):\n",
    "    W1, W2 = Theta\n",
    "    # compute activation\n",
    "    A = np.dot(W1, X)\n",
    "    # compute hidden unit output\n",
    "    H = 1 / (1 + np.exp(-A))\n",
    "    H[0] = 1\n",
    "    # compute network output\n",
    "    Y = np.dot(W2, H)\n",
    "    return Y, H"
   ],
   "id": "2150d0a3e703ce99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Gradient implementations",
   "id": "f22a5dd657852b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Gradient from formula\n",
    "\n",
    "For a given dataset $X$ the gradient of loss $J^{L_2}$ is defined as:\n",
    "\\begin{align}\n",
    "  \\frac{\\partial \\mathcal J}{\\partial w_{kd}^{(1)}} &= \\frac{2}{N} \\sum\\limits_{n=1}^N (y^{[n]}-t^{[n]}) w_{k}^{(2)} (1-h_{k}^{[n]}) h_{k}^{[n]} x_{d}^{[n]}\\\\\n",
    "  \\frac{\\partial \\mathcal J}{\\partial w_{k}^{(2)}} &= \\frac{2}{N} \\sum\\limits_{n=1}^N (y^{[n]}-t^{[n]}) h_{k}^{[n]}\n",
    "\\end{align}\n"
   ],
   "id": "14169bc9f16d0a16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 3,
   "source": [
    "def basic_gradient(X, Theta):\n",
    "    # split parameters for easier handling\n",
    "    W1, w2 = Theta\n",
    "    # define gradient with respect to both parameters\n",
    "    dW1 = np.zeros_like(W1)\n",
    "    dw2 = np.zeros_like(w2)\n",
    "    # iterate over dataset\n",
    "    for x, t in X:\n",
    "        # compute the gradient\n",
    "        y, h = simple_network(x, Theta)\n",
    "        dy = (y - t)\n",
    "        # compute gradient first layer\n",
    "        dh = dy * w2[1:] * (1 - h[1:]) * h[1:]\n",
    "        dW1 += 2 / len(X) * np.outer(dh, x)\n",
    "        # compute second layer\n",
    "        dw2 += 2 / len(X) * dy * h\n",
    "    return dW1, dw2\n",
    "\n",
    "\n",
    "def basic_gradient_descent(X, Theta, eta):\n",
    "    epochs = 10000\n",
    "    # perform iterative gradient descent\n",
    "    for epoch in range(epochs):\n",
    "        # compute the gradient\n",
    "        grad = basic_gradient(X, Theta)\n",
    "        # update the parameters\n",
    "        W1_new, w2_new = Theta\n",
    "        # update weights using gradient\n",
    "        W1_new -= eta * grad[0]\n",
    "        w2_new -= eta * grad[1]\n",
    "\n",
    "        # update Theta with new weights\n",
    "        Theta = (W1_new, w2_new)\n",
    "\n",
    "        if np.linalg.norm(grad[0]) < 1e-6:\n",
    "            break\n",
    "\n",
    "    # return optimized parameters\n",
    "    return Theta\n",
    "\n",
    "\n",
    "basic_gradient_descent(X1, Theta, eta=0.25)"
   ],
   "id": "45fcc7be4eda23eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Gradient - clever implementation (stochastic with batches)\n",
    "\n",
    "$\\nabla_{\\vec{w}^{(1)}}=\\frac{2}{B}\\sum\\limits_{b=1}^{B}[(y^{(b)}-t^{(b)})\\vec{w}^{[2]}\\odot\\vec{h}^{[b]}\\odot(1-\\vec{h}^{[b]})]\\otimes\\vec{x}^{[b]}$ \\\n",
    "$\\nabla_{\\vec{w}^{(2)}}=\\frac{2}{B}\\sum\\limits_{b=1}^{B}(y^{[b]}-t^{[b]})\\vec{h}^{[b]}$\n",
    "\n"
   ],
   "id": "45069dfade7c20e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clever_gradient(X, T, Y, H, Theta):\n",
    "    W1, W2 = Theta\n",
    "    dy = Y - T\n",
    "    # first layer gradient\n",
    "    g1 = (2 / Y.shape[1]) * np.dot((np.dot(W2.T, dy)) * H * (1 - H), X.T)\n",
    "    # second layer gradient\n",
    "    g2 = (2 / Y.shape[1]) * np.dot(dy, H.T)\n",
    "\n",
    "    return g1, g2\n",
    "\n",
    "\n",
    "def gradient_descent_with_batches(X, T, Theta, B, eta=0.001, mu=None):\n",
    "    loss_values = []\n",
    "\n",
    "    max_epochs = 10000\n",
    "    max_batches = T.shape[1] // B * max_epochs\n",
    "\n",
    "    # iterate over batches\n",
    "    for index, (x, t, e) in enumerate(batch_with_shuffle(X=X, T=T, batch_size=B)):\n",
    "        if index < max_batches:\n",
    "            # compute network output\n",
    "            y, h = multi_target_network(X=x, Theta=Theta)\n",
    "            # compute and append loss\n",
    "            if e:\n",
    "                loss_values.append(loss(Y=y, T=t))  # append loss of first batch\n",
    "\n",
    "            # compute gradient\n",
    "            g1, g2 = clever_gradient(X=x, T=t, Y=y, H=h, Theta=Theta)\n",
    "\n",
    "            # save previous theta for momentum\n",
    "            Theta_old = Theta\n",
    "\n",
    "            # and apply gradient descent\n",
    "            Theta[0] -= eta * g1\n",
    "            Theta[1] -= eta * g2\n",
    "\n",
    "            # apply momentum learning if desired\n",
    "            if mu:\n",
    "                Theta[0] += mu * (Theta[0] - Theta_old[0])\n",
    "                Theta[1] += mu * (Theta[1] - Theta_old[1])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # return the obtained loss values at the end\n",
    "    return loss_values\n",
    "\n",
    "\n",
    "# Stochastic gradient -> batch size\n",
    "SGD = gradient_descent_with_batches(X=X, T=T, Theta=Theta, B=16)"
   ],
   "id": "4af2689fb143129c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Classification",
   "id": "d30cf32048fd953d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### PyTorch",
   "id": "17c68090a2bc25a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tensor\n",
    "[torch.Tensor](https://pytorch.org/docs/stable/tensors.html)\n",
    "\n",
    "Most important attributes:\n",
    "* Initialize -> torch.Tensor()\n",
    "* Tensor.T -> returns the tensor with reversed dimensions\n",
    "* Tensor.shape -> returns the size (rows, columns)\n",
    "* Tensor.dtype -> return datatype of data in tensor\n",
    "* Tensor.device -> where the tensor is stored\n",
    "* Tensor.requires_grad -> whether a tensor requires gradient calculation during back propagation\n",
    "* Tensor.numel() -> total # of elements in tensor"
   ],
   "id": "8b0451be116f33d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Loss Function",
   "id": "e40e78ea4ddcee42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Binary Cross-Entropy loss\n",
    "Used for binary classification tasks"
   ],
   "id": "f0add98817a7b743"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "bce_loss = torch.nn.BCEWithLogitsLoss()",
   "id": "5d25ce5cf6e9f6ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Cross-Entropy loss\n",
    "Used for multi class classification tasks"
   ],
   "id": "cc347e5d84154e9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "ce_loss = torch.nn.CrossEntropyLoss()",
   "id": "61023141ab2c4a54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Network Examples",
   "id": "630546acb80df320"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# simple fully connected network with tanh\n",
    "def SiimpleFullyConnectedNetwork(D, K, O):\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(D, K),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(K, O)\n",
    "    )\n",
    "\n",
    "simple_fc_network = SiimpleFullyConnectedNetwork(X.shape[1], 10, 1)\n",
    "\n",
    "\n",
    "# 3 fully connected layers with flatten and sigmoid\n",
    "def TripleFullyConnectedNetwork(D, K1, K2, O):\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(D, K1),\n",
    "        torch.nn.Sigmoid(),\n",
    "        torch.nn.Linear(K1, K2),\n",
    "        torch.nn.Sigmoid(),\n",
    "        torch.nn.Linear(K2, O)\n",
    "    )\n",
    "\n",
    "triple_fc_network = TripleFullyConnectedNetwork(D=28*28, K1=128, K2=64, O=10)\n",
    "\n",
    "\n",
    "def DoubleConvoluionalNetwork(Q1, Q2, O):\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=1, out_channels=Q1, kernel_size=(7, 7), stride=1, padding=0),\n",
    "        torch.nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "        torch.nn.Sigmoid(),\n",
    "        torch.nn.Conv2d(in_channels=Q1, out_channels=Q2, kernel_size=(5, 5), stride=1, padding=2),\n",
    "        torch.nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "        torch.nn.Sigmoid(),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(5 * 5 * Q2, O)\n",
    "    )\n",
    "\n",
    "double_conv_network = DoubleConvoluionalNetwork(Q1=16, Q2=16, O=10)"
   ],
   "id": "d61877db42e5373e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Replace last network layer - Feature Extraction",
   "id": "6597831cff68a8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def replace_last_layer(network, O=6):\n",
    "    # replace the last linear layer with the new layer\n",
    "    num_of_in_features = network.fc.in_features\n",
    "    network.fc = torch.nn.Linear(num_of_in_features, O)\n",
    "    return network\n",
    "\n",
    "network_replaced_last_layer = replace_last_layer(simple_fc_network)  # Use network_2 defined above and replace the last layer"
   ],
   "id": "3f354937ebf49ca1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training Loops\n",
    "\n",
    "**IMPORTANT:** *do not forget to set the network to training mode `network.train()` and then to `network.eval()`"
   ],
   "id": "517ebe782996d16b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Basic\n",
    "With Stochastic Gradient Optimizer"
   ],
   "id": "8bcb3a6c342e34d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def basic_train(network, X_train, T_train, X_val, T_val, loss_function, learning_rate=0.1, epochs=10000):\n",
    "    optimizer = torch.optim.SGD(params=network.parameters(), lr=learning_rate)\n",
    "    # collect loss and accuracy values\n",
    "    train_loss, train_acc, val_loss, val_acc = [], [], [], []\n",
    "    for epoch in range(epochs):\n",
    "        # train on training set\n",
    "        optimizer.zero_grad()\n",
    "        # ... compute network output on training data\n",
    "        Z = network(X_train)\n",
    "        # ... compute loss from network output and target data\n",
    "        loss = loss_function(Z, T_train)\n",
    "        # ... perform parameter update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # ... remember loss\n",
    "        train_loss.append(loss.item())\n",
    "        # ... compute training set accuracy\n",
    "        train_acc.append(accuracy(Z, T_train).item())\n",
    "\n",
    "        # test on validation data\n",
    "        with torch.no_grad():\n",
    "            # ... compute network output on validation data\n",
    "            Z_v = network(X_val)\n",
    "            # ... compute loss from network output and target data\n",
    "            loss_v = loss_function(Z_v, T_val)\n",
    "            # ... remember loss\n",
    "            val_loss.append(loss_v.item())\n",
    "            # ... compute validation set accuracy\n",
    "            val_acc.append(accuracy(Z_v, T_val).item())\n",
    "\n",
    "    # return the four lists of losses and accuracies\n",
    "    return train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "# call basic train\n",
    "results = basic_train(network=simple_fc_network, X_train=X_train, T_train=T_train, X_val=X_val, T_val=T_val, loss_function=loss)"
   ],
   "id": "5c8ea07e484f4f24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### With batch loss",
   "id": "c6efb8c03049655a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def batch_train(network, epochs, eta, momentum):\n",
    "    # select loss function and optimizer\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(network.parameters(), lr=eta, momentum=momentum)\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    network = network.to(device)\n",
    "    # collect loss values and accuracies over the training epochs\n",
    "    val_loss, val_acc = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch \", epoch)\n",
    "        # train network on training data\n",
    "        for x, t in trainloader:\n",
    "            # put data to device\n",
    "            z = network(x.to(device))\n",
    "            # train\n",
    "            optimizer.zero_grad()\n",
    "            J = loss(z, t.to(device))\n",
    "            J.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # test network on test data\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            for x, t in testloader:\n",
    "                # put data to device\n",
    "                x = x.to(device)\n",
    "                t = t.to(device)\n",
    "                # compute validation loss\n",
    "                z = network(x)\n",
    "                J = loss(z, t)\n",
    "                # compute validation accuracy\n",
    "                correct += torch.sum(torch.argmax(z, dim=1) == t).item()\n",
    "                total_loss += J.item() * len(t)\n",
    "            acc = correct / len(testset)\n",
    "            avg_loss = total_loss / len(testset)\n",
    "            val_loss.append(avg_loss)\n",
    "            val_acc.append(acc)\n",
    "\n",
    "    # return loss and accuracy values\n",
    "    return val_loss, val_acc\n",
    "\n",
    "# call batch train\n",
    "fc_loss, fc_acc = batch_train(network=triple_fc_network, epochs=100, eta=0.01, momentum=0.9)\n",
    "cv_loss, cv_acc = batch_train(network=double_conv_network, epochs=100, eta=0.01, momentum=0.9)"
   ],
   "id": "c176d9004a9e6c3c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Train with evaluation",
   "id": "83460a5fcd257ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_eval(network, epochs=5, lr=0.001, momentum=0.9):\n",
    "    device = torch.device(\"mps\")\n",
    "    network.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(network.parameters(), lr=lr, momentum=momentum)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    total_train_loss, total_train_accuracy = 0, 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    train_loss, train_acc, val_loss, val_acc = 0, 0, 0, 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training process\n",
    "        network.train()\n",
    "        batch_train_loss, batch_train_accuracy = [], []\n",
    "        for x, t in trainloader:\n",
    "            x, t = x.to(device), t.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            J = loss(network(x), t)\n",
    "            J.backward()\n",
    "            optimizer.step()\n",
    "            batch_train_loss.append(J.item() * x.size(0))\n",
    "            batch_train_accuracy.append((network(x).argmax(dim=1) == 1).float().mean().item() * x.size(0))\n",
    "            total_samples += x.size(0)\n",
    "            \n",
    "            train_loss = sum(batch_train_loss) / total_samples\n",
    "            train_acc += sum(batch_train_accuracy) / total_samples\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}:\")\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "        # testing process\n",
    "        network.eval()\n",
    "        total_val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            batch_val_loss, batch_val_accuracy = [], []\n",
    "            for x, t in testloader:\n",
    "                x, t = x.to(device), t.to(device)\n",
    "                J = loss(network(x), t)\n",
    "                batch_val_loss.append(J.item() * x.size(0))\n",
    "                batch_val_accuracy.append((network(x).argmax(dim=1) == t).float().mean().item() * x.size(0))\n",
    "                total_val_samples += x.size(0)\n",
    "    \n",
    "        val_loss = sum(batch_val_loss) / total_val_samples\n",
    "        val_acc = sum(batch_val_accuracy) / total_val_samples\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # Save predictions and target labels of the test set after the last epoch\n",
    "    pred, target = [], []  #Store only the test results\n",
    "    with torch.no_grad():\n",
    "        for x, t in testloader:\n",
    "            x, t = x.to(device), t.to(device)\n",
    "            pred.append(network(x).argmax(dim=1).cpu().numpy())\n",
    "            target.append(t.cpu().numpy())\n",
    "    \n",
    "    return pred, target\n",
    "\n",
    "pred_unfrozen, targ_unfrozen = train_eval(network=network_replaced_last_layer)"
   ],
   "id": "8039fda013615cc0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Plotting",
   "id": "3ec618e959471fba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Line chart with two variables",
   "id": "12ded47c2f3ac990"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_line_chart_two_variables_with_range(X, Theta, R):\n",
    "    # create list from mult dimensional array\n",
    "    x_data = np.array([record[0] for record in X])\n",
    "    t_data = np.array([record[1] for record in X])\n",
    "    # first, plot data samples -> style 'x' as point\n",
    "    pyplot.plot(x_data[:, 1], t_data, \"rx\", label=\"Data\")\n",
    "    # define equidistant points from min (R[0]) to max (R[1]) to evaluate the network\n",
    "    x = np.arange(R[0], R[1], 100)\n",
    "    # compute the network outputs for these values\n",
    "    y = [simple_network(np.array([1, x_]), Theta)[0] for x_ in x]\n",
    "    # plot network approximation -> as a line\n",
    "    pyplot.plot(x, y, \"k-\", label=\"network\")\n",
    "    pyplot.legend()\n",
    "\n",
    "\n",
    "pyplot.subplot(131)\n",
    "plot_line_chart_two_variables_with_range(X1, None, [-1.5, 1.5])\n",
    "pyplot.title('Dataset X1')"
   ],
   "id": "d32757b3bece721c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Train vs validation loss",
   "id": "54bcabbe6b63211d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_train_vs_val_loss(train_loss, train_acc, val_loss, val_acc):\n",
    "    pyplot.figure(figsize=(10, 3))\n",
    "    ax = pyplot.subplot(121)\n",
    "    ax.plot(train_loss, \"g-\", label=\"Training set loss\")\n",
    "    ax.plot(val_loss, \"b-\", label=\"Validation set loss\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax = pyplot.subplot(122)\n",
    "    ax.plot(train_acc, \"g-\", label=\"Training set accuracy\")\n",
    "    ax.plot(val_acc, \"b-\", label=\"Validation set accuracy\")\n",
    "    ax.legend()"
   ],
   "id": "f07edc7d6a73c524"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### MNIST images plot",
   "id": "ae91c3b1630c0769"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pyplot.rcParams['image.cmap'] = 'gray'\n",
    "fig, axes = pyplot.subplots(4, 10, figsize=(10, 4))\n",
    "\n",
    "# 4 x 10 images\n",
    "index = 0\n",
    "for i in range(4):\n",
    "    for j in range(10):\n",
    "        img, _ = trainset[index]\n",
    "        axes[i][j].imshow(img, cmap='gray')\n",
    "        axes[i][j].axis(\"off\")\n",
    "        index += 1\n"
   ],
   "id": "f3bf3619972ee620"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Loss plots",
   "id": "ce5550bc4110588d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pyplot.figure(figsize=(10, 3))\n",
    "ax = pyplot.subplot(121)\n",
    "# plot loss values of FC and CV network over epochs\n",
    "ax.plot(fc_loss, \"g-\", label=\"Fully connected loss\")\n",
    "ax.plot(cv_loss, \"b-\", label=\"Convolutional loss\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Validation Loss\")\n",
    "\n",
    "ax = pyplot.subplot(122)\n",
    "# plot accuracy values of FC and CV network over epochs\n",
    "ax.plot(fc_acc, \"g-\", label=\"Fully connected accuracy\")\n",
    "ax.plot(cv_acc, \"b-\", label=\"Convolutional accuracy\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Validation Accuracy\")"
   ],
   "id": "754031415e79066f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Confusion Matrix",
   "id": "fe09a3190ae6b860"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "classes = trainset.classes\n",
    "\n",
    "pred_unfrozen_flat = np.concatenate(pred_unfrozen)\n",
    "targ_unfrozen_flat = np.concatenate(targ_unfrozen)\n",
    "\n",
    "# compute confusion matrix\n",
    "matrix_unfrozen = confusion_matrix(pred_unfrozen_flat, targ_unfrozen_flat)  # Use predictions and target from the fine-tuned network without frozen layers\n",
    "\n",
    "# plot confusion matrices\n",
    "plot_conf_matrix = ConfusionMatrixDisplay(matrix_unfrozen, display_labels=classes)\n",
    "plot_conf_matrix.plot(xticks_rotation=\"vertical\")\n",
    "plt.show()"
   ],
   "id": "b434c8c50f905645"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Theory",
   "id": "9d2008a6de109049"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Compute gradient\n",
    " -> See cheatsheet - [derivatives](Derivative%20Rules%20Cheatsheet.md)"
   ],
   "id": "2e3b64133dd4ba30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Convolutional Networks\n",
   "id": "fc719a3ca2ba4761"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Network output calculations\n",
    "\n",
    "***Output size of network -> (in + 2p - k) / s + 1***\n",
    "\n",
    "1. A [2D convolutional layer](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) with $Q_1$ channels, kernel size $7\\times7$, stride 1 and padding 0.\n",
    "2. A [2D maximum pooling](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) with pooling size $2\\times2$ and stride 2.\n",
    "3. A `Sigmoid` activation function.\n",
    "4. A 2D convolutional layer with $Q_2$ channels, kernel size $5\\times5$, stride 1 and padding 2.\n",
    "5. A 2D maximum pooling with pooling size $2\\times2$ and stride 2.\n",
    "6. A `Sigmoid` activation function.\n",
    "7. A flattening layer to turn the 3D feature map into a 1D vector.\n",
    "8. A fully-connected layer with the appropriate number of inputs and $O$ outputs.\n",
    "\n",
    "Consider the network as defined above\n",
    "Assume that the input is a $28\\times28$ grayscale image.\n",
    "How many hidden neurons do we need in the final fully-connected layer for a given number $Q_2$ of output channels of the second convolution?\n",
    "\n",
    "(Write steps of computation.)\n",
    "\n",
    "1. Input image size: $28\\times28$\n",
    "2. 1st Convolutional layer: $(28 + 2*0 - 7) / 1 + 1 = 22\\times22$\n",
    "3. 1st Max pooling layer: $(22/2)\\times(22/2) = 11\\times11$\n",
    "4. 1st Sigmoid activation function: no influence\n",
    "5. 2nd Convolutional layer: $(11 + 2*2 - 5) / 1 + 1 = 11\\times11$\n",
    "6. 2nd Max pooling layer: $(11/2)\\times(11/2) = 5.5\\times5.5$ -> round up: $6\\times6$\n",
    "7. 2nd Sigmoid activation function: no influence\n",
    "8. Flattening layer: $6\\times6 = 36$"
   ],
   "id": "a0588b59f7f6972"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Learnable parameters calculations\n",
    "\n",
    "1. A `torch.nn.Flatten` layer to turn the $28\\times28$ pixel image (2D) into a $28*28$ pixel vector (1D)\n",
    "2. A fully-connected layer with D input neurons and K1 outputs.\n",
    "3. A `Sigmoid` activation function.\n",
    " 4. A fully-connected layer with K1 input neurons and K2 outputs.\n",
    "5. A `Sigmoid`activation function.\n",
    "6. A fully-connected layer with K2 input neurons and O outputs.\n",
    "\n",
    "fully_connected(D=28*28, K1=128, K2=64, O=10)\n",
    "\n",
    "> #### Fully-connected Network:\n",
    ">\n",
    "> - first fully-connected layer: $(28*28+1)*128 =$ **100480**\n",
    "> - second fully-connected layer: $(128+1)*64 =$ **8256**\n",
    "> - third fully-connected layer: $(64+1)*10 =$ **650**\n",
    "> - total: $100480 + 8256 + 650 =$ **109386**\n",
    "\n",
    "1. A [2D convolutional layer](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) with $Q_1$ channels, kernel size $7\\times7$, stride 1 and padding 0.\n",
    "2. A [2D maximum pooling](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) with pooling size $2\\times2$ and stride 2.\n",
    "3. A `Sigmoid` activation function.\n",
    "4. A 2D convolutional layer with $Q_2$ channels, kernel size $5\\times5$, stride 1 and padding 2.\n",
    "5. A 2D maximum pooling with pooling size $2\\times2$ and stride 2.\n",
    "6. A `Sigmoid` activation function.\n",
    "7. A flattening layer to turn the 3D feature map into a 1D vector.\n",
    "8. A fully-connected layer with the appropriate number of inputs and $O$ outputs.\n",
    " \n",
    "convolutional(Q1=16, Q2=16, O=10)\n",
    "\n",
    "> #### Convolutional Network:\n",
    "> - first convolutional layer: $(7*7+1)*16$ = **800**\n",
    "> - second convolutional layer: $(16*5*5+1)*16$ = **6416**\n",
    "> - fully-connected layer: $(16*5*5+1)*10$ = **4010**\n",
    "> - total: $800 + 6416 + 4010 = **11226**\n",
    "\n",
    "***can check with torch.Tensor.numel()***"
   ],
   "id": "b39b784e92b47f73"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
