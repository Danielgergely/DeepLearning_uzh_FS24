{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Deep Learning Cheatsheet FS24\n",
   "id": "483fe10a0f91446e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Code snippets\n",
   "id": "1873f97552555977"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Imports",
   "id": "4834e20a66eb52c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "import copy\n",
    "import zipfile\n",
    "import csv\n",
    "import io\n",
    "import torch\n",
    "import csv\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import torchvision.models\n",
    "import torchvision.transforms as tfs\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates"
   ],
   "id": "d8605fa5a8c1ca12"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Device",
   "id": "b893fe746620a7ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# normal config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# apple silicon chip\n",
    "mps_device = torch.device(\"mps\")"
   ],
   "id": "b70a2c3190e0c9cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Target Vectors\n",
    "\n",
    "Classes definition\n",
    "\n",
    "* Known class indexes: (1, 4, 5, 8)\n",
    "* negative class indexes: (0, 2, 3, 7)\n",
    "* Unknown class indexes: (6,9)\n",
    "\n",
    "Target vector definition\n",
    " $\\vec t^n = 1 : \\vec t^n = (1,0,0,0)$\n",
    "\n",
    " $\\vec t^n = 4 : \\vec t^n = (0,1,0,0)$\n",
    "\n",
    " $\\vec t^n = 5 : \\vec t^n = (0,0,1,0)$\n",
    "\n",
    " $\\vec t^n = 8 : \\vec t^n = (0,0,0,1)$\n",
    "\n",
    " else: $\\vec t^n = (\\frac14,\\frac14,\\frac14,\\frac14)$\n"
   ],
   "id": "1edc12b27aae6286"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "known_classes = (1, 4, 5, 8)\n",
    "negative_classes = (0, 2, 3, 7)\n",
    "unknown_classes = (6, 9)\n",
    "O = len(known_classes)\n",
    "\n",
    "# define one-hot vectors\n",
    "labels_known = [torch.tensor([1, 0, 0, 0]),\n",
    "                torch.tensor([0, 1, 0, 0]),\n",
    "                torch.tensor([0, 0, 1, 0]),\n",
    "                torch.tensor([0, 0, 0, 1])]\n",
    "label_unknown = torch.tensor([0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "\n",
    "def target_vector(index):\n",
    "    # select correct one-hot vector for known classes, and the 1/O-vectors for unknown classes\n",
    "    if index in known_classes:\n",
    "        return labels_known[known_classes.index(index)]\n",
    "    else:\n",
    "        return label_unknown"
   ],
   "id": "b59d3392b576e55d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Tasks",
   "id": "d89c745cd641e0e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Data Generation\n",
    "\n",
    "Datasets: \n",
    "\n",
    "1. $X_1: t = \\sin(3x)$ for $x\\in[-1,1]$\n",
    "2. $X_2: t = e^{-4x^2}$ for $x\\in[-1,1]$\n",
    "3. $X_3: t = x^5 + 3x^4 - 6x^3 -12x^2 + 5x + 129$ for $x\\in[-4,2.5]$\n",
    "\n",
    "Generate dataset $X_1$, for $N=60$ samples randomly drawn from range $x\\in[-1,1]$. \\\n",
    "Generate data $X_2$ for $N=50$ samples randomly drawn from range $x\\in[-1,1]$.  \\\n",
    "Generate dataset $X_3$ for $N=200$ samples randomly drawn from range $x\\in[-4,2.5]$. \\\n",
    "Implement all three datasets as lists of tuples: $\\{(\\vec x^{[n]}, t^{[n]})\\mid 1\\leq n\\leq N\\}$. \\"
   ],
   "id": "2bac7f65208e28f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1D list\n",
    "np.random.uniform(low=-1, high=1, size=5 + 1)\n",
    "# 2D list\n",
    "np.random.uniform(low=-1, high=1, size=(5, 10))\n",
    "\n",
    "# special datasets\n",
    "X1 = [(np.array([1, x]), np.sin(3 * x)) for x in np.random.uniform(low=-1, high=1, size=60)]\n",
    "X2 = [(np.array([1, x]), np.exp(-4 * x ** 2)) for x in np.random.uniform(low=-1, high=1, size=50)]\n",
    "X3 = [(np.array([1, x]), x ** 5 + 3 * x ** 4 - 6 * x ** 3 - 12 * x ** 2 + 5 * x + 129) for x in\n",
    "      np.random.uniform(low=-4, high=2.5, size=200)]"
   ],
   "id": "843809aae7273e04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Read Data Examples",
   "id": "eab8c37021579517"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T10:19:57.828313Z",
     "start_time": "2024-06-11T10:19:57.821422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def data_from_zip_as_np_array(course=\"por\"):\n",
    "    # download data file from URL\n",
    "    dataset_zip_file = \"student.zip\"\n",
    "    if not os.path.exists(dataset_zip_file):\n",
    "        import urllib.request\n",
    "        urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00320/student.zip\",\n",
    "                                   dataset_zip_file)\n",
    "        print(\"Downloaded datafile\", dataset_zip_file)\n",
    "    # collect inputs\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    # some default values: yes=1, no=-1\n",
    "    yn = {\"yes\": 1., \"no\": -1.}\n",
    "    # read through dataset (without actually unzipping to a file):\n",
    "    # ... open zip file\n",
    "    zip = zipfile.ZipFile(dataset_zip_file)\n",
    "    # ... open data file inside of zip file and convert bytes to text\n",
    "    datafile = io.TextIOWrapper(zip.open(os.path.join(F\"student-{course}.csv\"), 'r'))\n",
    "    # ... read through the lines via CSV reader, using the correct delimiter\n",
    "    reader = csv.reader(datafile, delimiter=\";\")\n",
    "    # ... skip header line\n",
    "    next(reader)\n",
    "    for splits in reader:\n",
    "        # read input values\n",
    "        inputs.append([\n",
    "            1.,  #### BIAS ####\n",
    "            {\"GP\": 1., \"MS\": -1.}[splits[0]],  # school\n",
    "            float(splits[29]),  # absences\n",
    "        ])\n",
    "        # read targets values\n",
    "        targets.append([\n",
    "            float(splits[32]),  # grade for tertiary school\n",
    "        ])\n",
    "    print(F\"Loaded dataset with {len(targets)} samples\")\n",
    "    return np.array(inputs).transpose(), np.array(targets).transpose()\n",
    "\n",
    "\n",
    "def dataset_from_file(dataset_file=\"winequality-red.csv\", delimiter=\";\"):\n",
    "    # read dataset\n",
    "    with open(dataset_file, 'r') as f:\n",
    "        df = pd.read_csv(filepath_or_buffer=f, delimiter=delimiter, header=0)\n",
    "    # convert to torch.tensor\n",
    "    data = torch.tensor(df.values)\n",
    "    # get the input (data samples) without the target information\n",
    "    X = data[:, :-1].float()\n",
    "    if dataset_file == \"winequality-red.csv\":\n",
    "        # target is in the last column and needs to be converted to long\n",
    "        T = data[:, -1].long()\n",
    "        T = torch.sub(T, 3)\n",
    "    else:\n",
    "        # target is in the last column and needs to be of type float\n",
    "        T = data[:, -1].reshape(-1, 1).float()\n",
    "    return X, T\n",
    "\n",
    "\n",
    "def read_data_csv_pandas(datafile):\n",
    "    # Read/open datafile CSV file into a pandas DataFrame\n",
    "    data = pd.read_csv(datafile)\n",
    "    # Extract date and convert to numpy array\n",
    "    date = np.array(data['Date'], dtype=np.datetime64)\n",
    "    # Extract closing prices and convert to torch Tensor\n",
    "    price = torch.tensor(data['Close'].values, dtype=torch.float32)\n",
    "    return date, price\n",
    "\n",
    "gail_data = read_data_csv_pandas('./GAIL.csv')\n",
    "\n",
    "\n",
    "# intialize data\n",
    "X, T = data_from_zip_as_np_array(\"my_dataset\")"
   ],
   "id": "2e7ec7e7f1fbc2d2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Data initialization examples",
   "id": "37ef76057af1e5c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# data initialization\n",
    "K = 15\n",
    "D = len(X)\n",
    "O = 3\n",
    "# Weight initialization Xavier method\n",
    "W1 = np.random.uniform(low=-1 / np.sqrt(D), high=1 / np.sqrt(D), size=(K + 1, D))\n",
    "W2 = np.random.uniform(low=-1 / np.sqrt(K), high=1 / np.sqrt(K), size=(O, K + 1))\n",
    "Theta = [W1, W2]"
   ],
   "id": "2c120f417ffa4ee3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Data split",
   "id": "bc8acf0934958bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def split_training_data_with_shuffle(X, T, train_percentage=0.8, shuffle=True):\n",
    "    if shuffle:\n",
    "        # Combine X and T along axis 1\n",
    "        combined_data = np.concatenate((X, T), axis=1)\n",
    "        # Shuffle the combined data along axis 0\n",
    "        np.random.shuffle(combined_data)\n",
    "        # Split X and T again after shuffling\n",
    "        X = combined_data[:, :X.shape[1]]\n",
    "        T = combined_data[:, X.shape[1]:]\n",
    "\n",
    "    # split into 80/20 training/validation\n",
    "    training_number = int(X.shape[0] * train_percentage)\n",
    "    validation_numer = (X.shape[0] - training_number) * -1\n",
    "    X_train = X[:training_number]\n",
    "    T_train = T[:training_number]\n",
    "    X_val = X[validation_numer:]\n",
    "    T_val = T[validation_numer:]\n",
    "\n",
    "    return X_train, T_train, X_val, T_val\n",
    "\n",
    "X_train, T_train, X_val, T_val = split_training_data_with_shuffle(X=X, T=T)\n",
    "\n",
    "\n",
    "def train_test_split(stock_data):\n",
    "    dates, prices = stock_data\n",
    "    split_index = np.searchsorted(dates, np.datetime64('2018-01-01'))\n",
    "    train_data = prices[:split_index]\n",
    "    test_data = prices[split_index:]\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "gail_train, gail_test = train_test_split(gail_data)"
   ],
   "id": "c25e10ab2ccc47bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Batch split\n",
    "Utility function\n",
    "\n",
    "This function needs to return three elements:\n",
    "* First, the samples from the batch that belong to known classes.\n",
    "* Second, the target vectors that belong to the known classes.\n",
    "* Finally, the samples from the batch that belong to unknown classes."
   ],
   "id": "dec6873a783e05b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def split_known_unknown(batch, targets):\n",
    "    # select the indexes at which known and unknown samples exist\n",
    "    known = torch.any(targets == 1, dim=1)\n",
    "    unknown = torch.all(targets == 0.25, dim=1)\n",
    "    # return the known samples, the targets of the known samples, as well as the unknown samples\n",
    "    return batch[known], targets[known], batch[unknown]"
   ],
   "id": "e7e755b68219439f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Torch Datasets",
   "id": "a4118b4e55f2786c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Fashion MNIST dataset\n",
    "def f_mnist_datasets(transform):\n",
    "    trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    # returns PIL.Image.Image without transorm\n",
    "    return trainset, testset\n",
    "\n",
    "\n",
    "trainset, testset = f_mnist_datasets(transform=None)\n",
    "validationset = testset"
   ],
   "id": "e582353d974e55bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Data transform\n",
    "Convert images to tensors. First resize, then crop, then convert to tensor and normalize values"
   ],
   "id": "3214f2defffcb2cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# images to tensors\n",
    "imagenet_transform = tfs.Compose([\n",
    "    tfs.Resize(256),\n",
    "    tfs.CenterCrop(224),\n",
    "    tfs.ToTensor(),\n",
    "    tfs.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# to tensor transformer\n",
    "transform = torchvision.transforms.ToTensor()"
   ],
   "id": "69f3f7041f14c40e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Dataset loader - ImageFolder",
   "id": "19f9297b92e3e082"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dir = './intel-image-classification/seg_train/seg_train/'\n",
    "test_dir = './intel-image-classification/seg_test/seg_test/'\n",
    "\n",
    "trainset = ImageFolder(\n",
    "    root=train_dir,\n",
    "    transform=imagenet_transform\n",
    ")\n",
    "\n",
    "testset = ImageFolder(\n",
    "    root=test_dir,\n",
    "    transform=imagenet_transform\n",
    ")"
   ],
   "id": "ca119a53dc5a1b4f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Dataset Constructor\n",
    "A dataset class that derives from `torchvision.datasets.MNIST` in `PyTorch` and adapts some parts of it."
   ],
   "id": "9e9dde4902d89d77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DataSet(torchvision.datasets.MNIST):\n",
    "    def __init__(self, purpose=\"train\"):\n",
    "        # call base class constructor to handle the data loading\n",
    "        # make sure that you get the correct part of the data based on the purpose\n",
    "        super(DataSet, self).__init__(\n",
    "            root='./data',\n",
    "            train=True if purpose == \"train\" else False,\n",
    "            download=True,\n",
    "            transform=torchvision.transforms.ToTensor()\n",
    "        )\n",
    "        # select the valid classes based on the current purpose\n",
    "        if purpose == \"train\":\n",
    "            self.classes = known_classes + negative_classes\n",
    "        elif purpose == \"valid\":\n",
    "            self.classes = known_classes + negative_classes\n",
    "        else:\n",
    "            self.classes = known_classes + unknown_classes\n",
    "\n",
    "        # select the samples that belong to these classes\n",
    "        samples = np.column_stack([self.targets == c for c in self.classes]).any(axis=1)\n",
    "        # sub-select the data of valid classes\n",
    "        self.data = self.data[samples]\n",
    "        # select the targets of valid classes, and already turn them into target vectors\n",
    "        self.targets = self.targets[samples]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # perform appropriate actions on the data and the targets\n",
    "        # the format of data should be in [0, 1]\n",
    "        (input, target) = super().__getitem__(index)\n",
    "        target = target_vector(target)\n",
    "        return input, target"
   ],
   "id": "cdd8303ba2baef68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Torch Dataset - Simple",
   "id": "b37821f476b77e80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, S):\n",
    "        # store the data and targets as required\n",
    "        self.X, self.T = create_sequences_targets(data, S)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return input and target value for the given index\n",
    "        return torch.unsqueeze(self.X[index], -1), torch.unsqueeze(self.T[index], -1)\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the length of this dataset\n",
    "        return len(self.X)\n",
    "\n",
    "\n",
    "# instantiate dataset and data loader for a reasonable sequence length S\n",
    "S = 14\n",
    "train_gail_scaled = None\n",
    "gail_train_dataset = Dataset(train_gail_scaled, S)\n",
    "gail_train_dataloader = torch.utils.data.DataLoader(gail_train_dataset, batch_size=256, shuffle=True)"
   ],
   "id": "bc68419e6939d3db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Torch Dataset - MixedDataset",
   "id": "6004720801c2467b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MixedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root='./data', purpose=\"train\", transform=None, anomaly_size=2000):\n",
    "        # load MNIST dataset based on \"purpose\"\n",
    "        self.mnist_dataset = torchvision.datasets.MNIST(root=root, train=(purpose == \"train\"), download=True,\n",
    "                                                        transform=transform)\n",
    "\n",
    "        # load FashionMNIST dataset when \"purpose\" is \"anomaly_detection\" and randomly select samples with size \"anomaly_size\"\n",
    "        if purpose == \"anomaly_detection\":\n",
    "            fashion_mnist_dataset = torchvision.datasets.FashionMNIST(root=root, train=False, download=True,\n",
    "                                                                      transform=transform)\n",
    "            indices = np.random.choice(len(fashion_mnist_dataset), anomaly_size, replace=False)\n",
    "            self.fashion_mnist_dataset = torch.utils.data.Subset(fashion_mnist_dataset, indices)\n",
    "            self.data = torch.utils.data.ConcatDataset([self.mnist_dataset, self.fashion_mnist_dataset])\n",
    "        else:\n",
    "            self.data = self.mnist_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        # return length of the desired dataset based on its purpose\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # perform appropriate actions on the data, target, and its data type indicator (return 1 for regular and -1 for anomalous)\n",
    "        image = self.data[idx][0]\n",
    "        target = self.data[idx][1]\n",
    "        data_type = 1 if idx < len(self.mnist_dataset) else -1\n",
    "\n",
    "        return image, target, data_type"
   ],
   "id": "889eeb1d56a04462"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Data Loaders\n",
    "Data loaders simplify interaction with data. Shuffle data, create batches, etc. A bridge between dataset and model"
   ],
   "id": "2a5b1461fa7d0695"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "B = 512\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=B, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=B, shuffle=False)\n",
    "\n",
    "# instantiate anomaly detection dataset and data loader\n",
    "anomaly_detection_dataset = MixedDataset(purpose=\"anomaly_detection\", transform=transform, anomaly_size=2000)\n",
    "anomaly_detection_loader = torch.utils.data.DataLoader(anomaly_detection_dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "validationloader = torch.utils.data.DataLoader(validationset, batch_size=B, shuffle=False)"
   ],
   "id": "3ff46c5735edca3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Data Loader with MNIST data\n",
    "MNIST data is initialized directly in the constructor. "
   ],
   "id": "7b7843c7a522c942"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('data',\n",
    "                               train=True,\n",
    "                               download=True,\n",
    "                               transform=torchvision.transforms.ToTensor()),\n",
    "    batch_size=64,\n",
    "    shuffle=True)\n",
    "\n",
    "# validation set and data loader\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('data',\n",
    "                               train=False,\n",
    "                               download=True,\n",
    "                               transform=torchvision.transforms.ToTensor()),\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")"
   ],
   "id": "1d71522abd51be14"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Normalize",
   "id": "89ea684ca451f3da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# get min and max values\n",
    "min_val = np.max(X, axis=1)\n",
    "max_val = np.min(X, axis=1)\n",
    "\n",
    "# assure to handle x_0 correctly\n",
    "min_val[0] = 0\n",
    "max_val[0] = 1\n",
    "\n",
    "def normalize(x, min_val, max_val):\n",
    "    # normalize the given data with the given minimum and maximum values\n",
    "    return np.transpose((x.transpose() - min_val) / (max_val - min_val))\n",
    "\n",
    "\n",
    "# Normalize our dataset\n",
    "X = normalize(X, min_val, max_val)\n",
    "\n",
    "\n",
    "########### Normalize min-max scaler ###################\n",
    "def min_max_scaler(train_data, test_data):\n",
    "    # Compute the correct statistics\n",
    "    min_train = torch.min(train_data)\n",
    "    max_train = torch.max(train_data)\n",
    "    min_test = torch.min(test_data)\n",
    "    max_test = torch.max(test_data)\n",
    "    min_val = torch.min(min_train, min_test)\n",
    "    max_val = torch.max(max_train, max_test)\n",
    "\n",
    "    # Scale the training data\n",
    "    train_data_scaled = (train_data - min_val) / (max_val - min_val)\n",
    "    # Scale the test data using the same min and max values\n",
    "    test_data_scaled = (test_data - min_val) / (max_val - min_val)\n",
    "\n",
    "    return train_data_scaled, test_data_scaled, min_val, max_val\n",
    "\n",
    "\n",
    "def inverse_min_max_scaler(scaled_data, min_val, max_val):\n",
    "    # Revert the scaling\n",
    "    original_data = scaled_data * (max_val - min_val) + min_val\n",
    "\n",
    "    return original_data\n",
    "\n",
    "# scale and un-scale data\n",
    "train_gail_scaled, test_gail_scaled, min_gail, max_gail = min_max_scaler(gail_train, gail_test)\n",
    "reversed_gail_data = inverse_min_max_scaler(train_gail_scaled, min_val, max_val)"
   ],
   "id": "2f455a0ddaa0ce3c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Standardize",
   "id": "2fd5446c98f07c1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def standardize(X_train, X_val):\n",
    "    # compute statistics\n",
    "    mean = torch.mean(X_train, dim=0)\n",
    "    std = torch.std(X_train, dim=0)\n",
    "\n",
    "    # standardize both X_train and X_val\n",
    "    X_train = (X_train - mean) / std\n",
    "    X_val = (X_val - mean) / std\n",
    "    return X_train, X_val\n",
    "\n",
    "\n",
    "X_train, X_val = standardize(X_train=X_train, X_val=X_val)"
   ],
   "id": "f696e414ad7b5559"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Accuracy\n",
    "Accuracy check for categorical or binary classification"
   ],
   "id": "3a43e8f857015b81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def accuracy(Z, T):\n",
    "    # check if we have binary or categorical classification\n",
    "    if len(T.shape) == 2:\n",
    "        # binary classification\n",
    "        y = (Z >= 0).float()\n",
    "        return torch.mean((y == T).float())\n",
    "    else:\n",
    "        # categorical classification\n",
    "        y = torch.argmax(Z, dim=1)\n",
    "        return torch.mean((y == T).float())"
   ],
   "id": "722a14c8ed44edf2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Confidence\n",
    "\n",
    "The function computes the confidence value for a given batch of samples"
   ],
   "id": "ec6bffb979b37ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def confidence(logits, targets):\n",
    "    # compute softmax confidences\n",
    "    conf = torch.nn.functional.softmax(logits, dim=1)\n",
    "    # split between known and unknown\n",
    "    batch_known, targets_known, batch_unknown = split_known_unknown(conf, targets)\n",
    "    # compute confidence score for known targets\n",
    "    conf_known = torch.sum(torch.max(batch_known, dim=1)[0])\n",
    "    # compute confidence score for unknown targets\n",
    "    conf_unknown = torch.sum(1 - torch.max(batch_unknown, dim=1)[0] + 1 / O)\n",
    "    return conf_known + conf_unknown"
   ],
   "id": "b1304b6a0669b7a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### True Positive/Negative rate calculation",
   "id": "3dd977b4e606286f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_tpr_tnr(predictions, truth):\n",
    "    # convert list into numpy array\n",
    "    predictions = np.array(predictions)\n",
    "    truth = np.array(truth)\n",
    "    # Compute the confusion matrix or tp, tn, fp, fn\n",
    "    tn, fp, fn, tp = confusion_matrix(truth, predictions).ravel()\n",
    "    # Compute TPR and TNR\n",
    "    tpr = tp / (tp + fn)\n",
    "    tnr = tn / (tn + fp)\n",
    "\n",
    "    return tpr, tnr"
   ],
   "id": "f9eff953f8bf4430"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create sequence",
   "id": "2ce11c94d8e7ca2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_sequences_targets(data: torch.Tensor, S):\n",
    "    # Initialize empty lists to hold the input sequences and the corresponding target values\n",
    "    X, T = [], []\n",
    "    # Go through the data to extract sequences based on S\n",
    "    for i in range(len(data) - S):\n",
    "        X.append(data[i:i + S])\n",
    "        T.append(data[i + S])\n",
    "\n",
    "    # Convert lists of sequences and targets into PyTorch tensors\n",
    "    return torch.stack(X), torch.stack(T)"
   ],
   "id": "f1d37e69bc511d2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Item prediction\n",
    "Requiers a network which has been trained using the train data, and the test data which it than uses to predict the next sequence items"
   ],
   "id": "ddd4d86e446bb9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def predict(network, test_dataloader):\n",
    "    network.eval()\n",
    "    predictions = []\n",
    "\n",
    "    for x, _ in test_dataloader:\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            y = network(x)\n",
    "            predictions.append(y)\n",
    "            \n",
    "    return torch.cat(predictions).cpu()"
   ],
   "id": "6a37d73df1d1d752"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Network implementations",
   "id": "bce3c790cf6714ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Activation functions",
   "id": "a828fb435033d682"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def logistic(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ],
   "id": "6027a4489d3f319c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Loss Functions",
   "id": "829acabb1995aa27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Squared Loss\n",
    "$\\mathcal J^{L_2} = \\frac1B \\|\\mathbf Y - \\mathbf T\\|_F^2$ for given network outputs $\\mathbf Y$ and target values $\\mathbf T$."
   ],
   "id": "93c10704b6e879e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def loss(Y, T):\n",
    "    return (1 / T.shape[1]) * np.linalg.norm(Y - T, \"fro\") ** 2"
   ],
   "id": "96e5fcb40ac81a1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Batch creation",
   "id": "4a3e1b774e56f07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 5,
   "source": [
    "# used in enumerate -> yield\n",
    "def batch_with_shuffle(X, T, batch_size=16):\n",
    "    num_of_samples = X.shape[1]\n",
    "    shuffle_idx = np.random.permutation(num_of_samples)\n",
    "    i = 0\n",
    "    new_epoch = True\n",
    "    while True:\n",
    "        # shuffle dataset in each epoch   \n",
    "        if (i + batch_size) >= X.shape[1]:\n",
    "            shuffle_idx = np.random.permutation(X.shape[1])\n",
    "            i = 0\n",
    "            new_epoch = True\n",
    "        # yield the batch\n",
    "        yield X[:, shuffle_idx[i:i + batch_size]], T[:, shuffle_idx[i:i + batch_size]], new_epoch\n",
    "        new_epoch = False\n",
    "        i += batch_size"
   ],
   "id": "9a415197e18dc2bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Network examples",
   "id": "db36fd6cbb896f06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 4,
   "source": [
    "# Network for a given input vector and parameters Theta\n",
    "def simple_network(x, Theta):\n",
    "    W1, w2 = Theta\n",
    "    # linear combination for hidden layer\n",
    "    a_ = np.dot(W1, x)\n",
    "    # activation function\n",
    "    h_ = logistic(a_)\n",
    "    # adding bias\n",
    "    h = np.insert(h_, 0, 1)\n",
    "    y = np.dot(w2, h)\n",
    "    return y, h\n",
    "\n",
    "\n",
    "# Multi-target network -> output matrix Y, hidden unit output H\n",
    "def multi_target_network(X, Theta):\n",
    "    W1, W2 = Theta\n",
    "    # compute activation\n",
    "    A = np.dot(W1, X)\n",
    "    # compute hidden unit output\n",
    "    H = 1 / (1 + np.exp(-A))\n",
    "    H[0] = 1\n",
    "    # compute network output\n",
    "    Y = np.dot(W2, H)\n",
    "    return Y, H"
   ],
   "id": "2150d0a3e703ce99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Gradient implementations",
   "id": "f22a5dd657852b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Gradient from formula\n",
    "\n",
    "For a given dataset $X$ the gradient of loss $J^{L_2}$ is defined as:\n",
    "\\begin{align}\n",
    "  \\frac{\\partial \\mathcal J}{\\partial w_{kd}^{(1)}} &= \\frac{2}{N} \\sum\\limits_{n=1}^N (y^{[n]}-t^{[n]}) w_{k}^{(2)} (1-h_{k}^{[n]}) h_{k}^{[n]} x_{d}^{[n]}\\\\\n",
    "  \\frac{\\partial \\mathcal J}{\\partial w_{k}^{(2)}} &= \\frac{2}{N} \\sum\\limits_{n=1}^N (y^{[n]}-t^{[n]}) h_{k}^{[n]}\n",
    "\\end{align}\n"
   ],
   "id": "14169bc9f16d0a16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 3,
   "source": [
    "def basic_gradient(X, Theta):\n",
    "    # split parameters for easier handling\n",
    "    W1, w2 = Theta\n",
    "    # define gradient with respect to both parameters\n",
    "    dW1 = np.zeros_like(W1)\n",
    "    dw2 = np.zeros_like(w2)\n",
    "    # iterate over dataset\n",
    "    for x, t in X:\n",
    "        # compute the gradient\n",
    "        y, h = simple_network(x, Theta)\n",
    "        dy = (y - t)\n",
    "        # compute gradient first layer\n",
    "        dh = dy * w2[1:] * (1 - h[1:]) * h[1:]\n",
    "        dW1 += 2 / len(X) * np.outer(dh, x)\n",
    "        # compute second layer\n",
    "        dw2 += 2 / len(X) * dy * h\n",
    "    return dW1, dw2\n",
    "\n",
    "\n",
    "def basic_gradient_descent(X, Theta, eta):\n",
    "    epochs = 10000\n",
    "    # perform iterative gradient descent\n",
    "    for epoch in range(epochs):\n",
    "        # compute the gradient\n",
    "        grad = basic_gradient(X, Theta)\n",
    "        # update the parameters\n",
    "        W1_new, w2_new = Theta\n",
    "        # update weights using gradient\n",
    "        W1_new -= eta * grad[0]\n",
    "        w2_new -= eta * grad[1]\n",
    "\n",
    "        # update Theta with new weights\n",
    "        Theta = (W1_new, w2_new)\n",
    "\n",
    "        if np.linalg.norm(grad[0]) < 1e-6:\n",
    "            break\n",
    "\n",
    "    # return optimized parameters\n",
    "    return Theta\n",
    "\n",
    "\n",
    "basic_gradient_descent(X1, Theta, eta=0.25)"
   ],
   "id": "45fcc7be4eda23eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Gradient - clever implementation (stochastic with batches)\n",
    "\n",
    "$\\nabla_{\\vec{w}^{(1)}}=\\frac{2}{B}\\sum\\limits_{b=1}^{B}[(y^{(b)}-t^{(b)})\\vec{w}^{[2]}\\odot\\vec{h}^{[b]}\\odot(1-\\vec{h}^{[b]})]\\otimes\\vec{x}^{[b]}$ \\\n",
    "$\\nabla_{\\vec{w}^{(2)}}=\\frac{2}{B}\\sum\\limits_{b=1}^{B}(y^{[b]}-t^{[b]})\\vec{h}^{[b]}$\n",
    "\n"
   ],
   "id": "45069dfade7c20e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clever_gradient(X, T, Y, H, Theta):\n",
    "    W1, W2 = Theta\n",
    "    dy = Y - T\n",
    "    # first layer gradient\n",
    "    g1 = (2 / Y.shape[1]) * np.dot((np.dot(W2.T, dy)) * H * (1 - H), X.T)\n",
    "    # second layer gradient\n",
    "    g2 = (2 / Y.shape[1]) * np.dot(dy, H.T)\n",
    "\n",
    "    return g1, g2\n",
    "\n",
    "\n",
    "def gradient_descent_with_batches(X, T, Theta, B, eta=0.001, mu=None):\n",
    "    loss_values = []\n",
    "\n",
    "    max_epochs = 10000\n",
    "    max_batches = T.shape[1] // B * max_epochs\n",
    "\n",
    "    # iterate over batches\n",
    "    for index, (x, t, e) in enumerate(batch_with_shuffle(X=X, T=T, batch_size=B)):\n",
    "        if index < max_batches:\n",
    "            # compute network output\n",
    "            y, h = multi_target_network(X=x, Theta=Theta)\n",
    "            # compute and append loss\n",
    "            if e:\n",
    "                loss_values.append(loss(Y=y, T=t))  # append loss of first batch\n",
    "\n",
    "            # compute gradient\n",
    "            g1, g2 = clever_gradient(X=x, T=t, Y=y, H=h, Theta=Theta)\n",
    "\n",
    "            # save previous theta for momentum\n",
    "            Theta_old = Theta\n",
    "\n",
    "            # and apply gradient descent\n",
    "            Theta[0] -= eta * g1\n",
    "            Theta[1] -= eta * g2\n",
    "\n",
    "            # apply momentum learning if desired\n",
    "            if mu:\n",
    "                Theta[0] += mu * (Theta[0] - Theta_old[0])\n",
    "                Theta[1] += mu * (Theta[1] - Theta_old[1])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # return the obtained loss values at the end\n",
    "    return loss_values\n",
    "\n",
    "\n",
    "# Stochastic gradient -> batch size\n",
    "SGD = gradient_descent_with_batches(X=X, T=T, Theta=Theta, B=16)"
   ],
   "id": "4af2689fb143129c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Classification",
   "id": "d30cf32048fd953d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### PyTorch",
   "id": "17c68090a2bc25a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tensor\n",
    "[torch.Tensor](https://pytorch.org/docs/stable/tensors.html)\n",
    "\n",
    "Most important attributes:\n",
    "* Initialize -> torch.Tensor()\n",
    "* Tensor.T -> returns the tensor with reversed dimensions\n",
    "* Tensor.shape -> returns the size (rows, columns)\n",
    "* Tensor.dtype -> return datatype of data in tensor\n",
    "* Tensor.device -> where the tensor is stored\n",
    "* Tensor.requires_grad -> whether a tensor requires gradient calculation during back propagation\n",
    "* Tensor.numel() -> total # of elements in tensor"
   ],
   "id": "8b0451be116f33d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Loss Function",
   "id": "e40e78ea4ddcee42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Binary Cross-Entropy loss\n",
    "Used for binary classification tasks"
   ],
   "id": "f0add98817a7b743"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "bce_loss = torch.nn.BCEWithLogitsLoss()",
   "id": "5d25ce5cf6e9f6ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Cross-Entropy loss\n",
    "Used for multi class classification tasks"
   ],
   "id": "cc347e5d84154e9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "ce_loss = torch.nn.CrossEntropyLoss()",
   "id": "61023141ab2c4a54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Mean-Squared error loss\n",
    "Best used when the goal is to predict continuous values (regression tasks)"
   ],
   "id": "25dc7fec6cc6cf08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "loss = torch.nn.MSELoss()",
   "id": "3ff21f69a9529433"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Network Examples",
   "id": "630546acb80df320"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Sequential",
   "id": "b9ab244ade5d9a59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# simple fully connected network with tanh\n",
    "def SiimpleFullyConnectedNetwork(D, K, O):\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(D, K),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(K, O)\n",
    "    )\n",
    "\n",
    "\n",
    "simple_fc_network = SiimpleFullyConnectedNetwork(X.shape[1], 10, 1)\n",
    "\n",
    "\n",
    "# 3 fully connected layers with flatten and sigmoid\n",
    "def TripleFullyConnectedNetwork(D, K1, K2, O):\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(D, K1),\n",
    "        torch.nn.Sigmoid(),\n",
    "        torch.nn.Linear(K1, K2),\n",
    "        torch.nn.Sigmoid(),\n",
    "        torch.nn.Linear(K2, O)\n",
    "    )\n",
    "\n",
    "\n",
    "triple_fc_network = TripleFullyConnectedNetwork(D=28 * 28, K1=128, K2=64, O=10)\n",
    "\n",
    "\n",
    "def DoubleConvoluionalNetwork(Q1, Q2, O):\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=1, out_channels=Q1, kernel_size=(7, 7), stride=1, padding=0),\n",
    "        torch.nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "        torch.nn.Sigmoid(),\n",
    "        torch.nn.Conv2d(in_channels=Q1, out_channels=Q2, kernel_size=(5, 5), stride=1, padding=2),\n",
    "        torch.nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "        torch.nn.Sigmoid(),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(5 * 5 * Q2, O)\n",
    "    )\n",
    "\n",
    "\n",
    "double_conv_network = DoubleConvoluionalNetwork(Q1=16, Q2=16, O=10)"
   ],
   "id": "d61877db42e5373e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Module\n",
    "\n",
    "The topology can be found in the following:\n",
    "1. 2D convolutional layer with $Q_1$ channels, kernel size $7\\times7$, stride 1 and padding 0\n",
    "2. 2D maximum pooling layer with kernel size $2\\times2$ and stride 2\n",
    "3. activation function **PReLU**\n",
    "4. 2D convolutional layer with $Q_2$ channels, kernel size $5\\times5$, stride 1 and padding 2\n",
    "5. 2D maximum pooling layer with kernel size $2\\times2$ and stride 2\n",
    "6. activation function **PReLU**\n",
    "7. flatten layer to convert the convolution output into a vector\n",
    "8. fully-connected layer with the correct number of inputs and $K$ outputs\n",
    "9. fully-connected layer with $K$ inputs and $O$ outputs"
   ],
   "id": "c5a35c6500a01bca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ModuleNetwork(torch.nn.Module):\n",
    "    def __init__(self, Q1, Q2, K, O):\n",
    "        # call base class constructor\n",
    "        super(ModuleNetwork, self).__init__()\n",
    "        # define convolutional layers\n",
    "        self.conv1 = torch.nn.Conv2d(1, Q1, kernel_size=(7, 7), stride=1, padding=0)\n",
    "        self.conv2 = torch.nn.Conv2d(Q1, Q2, kernel_size=(5, 5), stride=1, padding=2)\n",
    "        # pooling and activation functions will be re-used for the different stages\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        self.act = torch.nn.PReLU()\n",
    "        # define fully-connected layers\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc1 = torch.nn.Linear(Q2 * 5 * 5, K)\n",
    "        self.fc2 = torch.nn.Linear(K, O)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # compute first layer of convolution, pooling and activation\n",
    "        a = self.act(self.pool(self.conv1(x)))\n",
    "        # compute second layer of convolution, pooling and activation\n",
    "        a = self.act(self.pool(self.conv2(a)))\n",
    "        # get the deep features as the output of the first fully-connected layer\n",
    "        deep_features = self.fc1(self.flatten(a))\n",
    "        # get the logits as the output of the second fully-connected layer\n",
    "        logits = self.fc2(deep_features)\n",
    "        # return both the logits and the deep features\n",
    "        return logits, deep_features\n",
    "\n",
    "\n",
    "# initiate class\n",
    "module_network_adapted = ModuleNetwork(32, 32, 20, 4)"
   ],
   "id": "eceabb178c4e78c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Encoder Network Module\n",
    "\n",
    "(a) Encoder Network\n",
    "\n",
    "*   2D convolutional layer with $Q_1$ output channels, kernel size $5\\times5$, **stride 2** and padding 2\n",
    "*   activation function ReLU\n",
    "*   2D convolutional layer with $Q_2$ output channels, kernel size $5\\times5$, **stride 2** and padding 2\n",
    "*   flatten layer to convert the convolution output into a vector\n",
    "*   activation function ReLU\n",
    "*   fully-connected layer with the correct number of inputs and $K$ outputs"
   ],
   "id": "3558be8bda9083f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, Q1, Q2, K):\n",
    "        # call base class constrcutor\n",
    "        super(Encoder, self).__init__()\n",
    "        # convolutional define layers\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=Q1, kernel_size=5, stride=2, padding=2)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=Q1, out_channels=Q2, kernel_size=5, stride=2, padding=2)\n",
    "        # activation functions will be re-used for the different stages\n",
    "        self.act = torch.nn.ReLU()\n",
    "        # define fully-connected layers\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc = torch.nn.Linear(Q2 * 7 * 7, K)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get the deep feature representation\n",
    "        a = self.act(self.conv1(x))\n",
    "        a = self.flatten(self.conv2(a))\n",
    "        deep_feature = self.fc(self.act(a))\n",
    "        return deep_feature"
   ],
   "id": "f9a05461805f68d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "(b) Encoder (Decoder) Network\n",
    "\n",
    "*   fully-connected layer with $K$ inputs and the correct number of outputs\n",
    "*   activation function ReLU\n",
    "*   reshaping to convert the vector into a convolution input\n",
    "*   2D **fractionally-strided convolutional** layer with $Q_2$ input channels, kernel size $5\\times5$, stride 2 and padding 2\n",
    "*   activation function ReLU\n",
    "*   2D **fractionally-strided convolutional** layer with $Q_1$ input channels, kernel size $5\\times5$, stride 2 and padding 2"
   ],
   "id": "c859548752ffe53d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, Q1, Q2, K):\n",
    "        # call base class constrcutor\n",
    "        super(Decoder, self).__init__()\n",
    "        # fully-connected layer\n",
    "        self.fc = torch.nn.Linear(K, Q2 * 7 * 7)\n",
    "        # convolutional layers\n",
    "        self.deconv1 = torch.nn.ConvTranspose2d(in_channels=Q2, out_channels=Q1, kernel_size=5, stride=2, padding=2,\n",
    "                                                output_padding=1)\n",
    "        self.deconv2 = torch.nn.ConvTranspose2d(in_channels=Q1, out_channels=1, kernel_size=5, stride=2, padding=2,\n",
    "                                                output_padding=1)\n",
    "        # activation function\n",
    "        self.act = torch.nn.ReLU()\n",
    "        # unflatten\n",
    "        self.unflatten = torch.nn.Unflatten(1, (Q2, 7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # reconstruct the output image\n",
    "        a = self.unflatten(self.act(self.fc(x)))\n",
    "        a = self.act(self.deconv1(a))\n",
    "        output = torch.sigmoid(self.deconv2(a))\n",
    "        return output"
   ],
   "id": "779f12262e799a89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Combine the two",
   "id": "1d71c82887b9be5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, Q1, Q2, K):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder(Q1, Q2, K)\n",
    "        self.decoder = Decoder(Q1, Q2, K)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode input\n",
    "        deep_feature = self.encoder(x)\n",
    "        # decode to output\n",
    "        reconstructed = self.decoder(deep_feature)\n",
    "        return reconstructed"
   ],
   "id": "e1976a25a91831ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Replace last network layer - Feature Extraction",
   "id": "6597831cff68a8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def replace_last_layer(network, O=6):\n",
    "    # replace the last linear layer with the new layer\n",
    "    num_of_in_features = network.fc.in_features\n",
    "    network.fc = torch.nn.Linear(num_of_in_features, O)\n",
    "    return network\n",
    "\n",
    "\n",
    "network_replaced_last_layer = replace_last_layer(\n",
    "    simple_fc_network)  # Use network_2 defined above and replace the last layer"
   ],
   "id": "3f354937ebf49ca1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### LSTM",
   "id": "4f160004f964c139"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class LSTMModel(torch.nn.Module):\n",
    "    def __init__(self, D, K, O):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(D, K, batch_first=True)\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.linear = torch.nn.Linear(K, O)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # lstm layer\n",
    "        output, _ = self.lstm(x)\n",
    "        # apply dropout to the output of lstm layer\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        # get correct element of the output of the lstm layer\n",
    "        last_output = output[:, -1, :]\n",
    "        Z = self.linear(last_output)\n",
    "\n",
    "        return Z\n",
    "    \n",
    "lstm_gail_network = LSTMModel(D, K, O)"
   ],
   "id": "dd8f8f7dd830f794"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Small-scale Network\n",
    "A network with two convolutional and two fully-connected layers.\n",
    "The first convolutional layer has kernel size $7 \\times 7$, stride $=1$, and padding $=0$. The second one has kernel size $5\\times5$, stride $=1$, and padding $=2$. Both are followed by a $2\\times2$ maximum pooling and a ReLU activation."
   ],
   "id": "82cf0fbab4140185"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, Q1, Q2, K, O):\n",
    "        # call base class constructor\n",
    "        super(Network, self).__init__() # -> output = (input - kernel_size + 2 * padding) / stride + 1\n",
    "        self.conv1 = torch.nn.Conv2d(1, Q1, 7, stride=1, padding=0)\n",
    "        self.conv2 = torch.nn.Conv2d(Q1, Q2, 5, stride=1, padding=2)\n",
    "        self.pool = torch.nn.MaxPool2d(2)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc1 = torch.nn.Linear(Q2 * 5 * 5, K)\n",
    "        self.fc2 = torch.nn.Linear(K, O)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.pool(self.conv1(x))) # -> (28 - 7 + 2 * 0) / 1 + 1 = 22 -> 22 / 2 = 11 -> 11\n",
    "        x = self.act(self.pool(self.conv2(x))) # -> (11 - 5 + 2 * 2) / 1 + 1 = 11 -> 11 / 2 = 5.5 -> 5\n",
    "        x = self.flatten(x) # -> 5 * 5 * Q2\n",
    "        return self.fc2(self.fc1(x))"
   ],
   "id": "525e4c402f0165fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Validation loop\n",
    "\n",
    "For a given network and loss function, this function iterates over the validation set and computes the classification accuracy on the original validation set samples.\n",
    "\n",
    "For each batch, select the correctly classified images. For these, generate two types of adversarial samples, using FGS and FGV defined above, respectively.\n",
    "\n",
    "Finally, compute how many of the adversarial samples are still classified as the original class by the network."
   ],
   "id": "66a40a08b82c8230"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def validation_loop(network, loss, alpha_fgs=0.3, alpha_fgv=0.6):\n",
    "    total, correct_clean_count, correct_fgs_count, correct_fgv_count = 0, 0, 0, 0\n",
    "    \n",
    "    network = network.to(device)\n",
    "    network.eval()\n",
    "\n",
    "    # iterate over validation set samples\n",
    "    for x, t in validation_loader:\n",
    "        x, t = x.to(device), t.to(device)\n",
    "        total += x.shape[0]\n",
    "        with torch.no_grad():\n",
    "            # classify original samples\n",
    "            z = network(x)\n",
    "\n",
    "            # compute classification accuracy on original samples\n",
    "            _, predicted = torch.max(z.data, 1)\n",
    "            correct_clean_count += (predicted == t).sum().item()\n",
    "\n",
    "        # select the correctly classified samples\n",
    "        correct_indices = (predicted == t)\n",
    "\n",
    "        # create adversarial samples using FGS and FGV only if x_correct is not empty\n",
    "        if correct_indices.sum().item() > 0:\n",
    "            x_correct, t_correct = x[correct_indices], t[correct_indices]\n",
    "            x_attack_fgs = FGS(x_correct, t_correct, network, loss, alpha=alpha_fgs)\n",
    "            x_attack_fgv = FGV(x_correct, t_correct, network, loss, alpha=alpha_fgv)\n",
    "\n",
    "            # check how many are correctly classified\n",
    "            with torch.no_grad():\n",
    "                # classify adversarial samples\n",
    "                z_attack_fgs = network(x_attack_fgs)\n",
    "                z_attack_fgv = network(x_attack_fgv)\n",
    "\n",
    "                # compute classification accuracy on adversarial samples\n",
    "                _, predicted_fgs = torch.max(z_attack_fgs.data, 1)\n",
    "                _, predicted_fgv = torch.max(z_attack_fgv.data, 1)\n",
    "                correct_fgs_count += (predicted_fgs == t_correct).sum().item()\n",
    "                correct_fgv_count += (predicted_fgv == t_correct).sum().item()\n",
    "\n",
    "    # compute clean and adversarial accuracies and return them\n",
    "    clean_accuracy = correct_clean_count / total\n",
    "    fgs_accuracy = correct_fgs_count / correct_clean_count\n",
    "    fgv_accuracy = correct_fgv_count / correct_clean_count\n",
    "    print(\"Clean acc: \", clean_accuracy, \" , FGS acc: \", fgs_accuracy, \" , FGV acc: \", fgv_accuracy)\n",
    "    return clean_accuracy, fgs_accuracy, fgv_accuracy"
   ],
   "id": "21bd96d983940b05"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Autograd",
   "id": "b32f5de1b51e1d8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class AdaptedSoftMax(torch.autograd.Function):\n",
    "    # implement the forward propagation\n",
    "    @staticmethod\n",
    "    def forward(ctx, logits, targets):\n",
    "        # compute the log probabilities via log_softmax\n",
    "        log_probabilities = torch.nn.functional.log_softmax(logits, dim=1)\n",
    "        # save required values for backward pass\n",
    "        ctx.save_for_backward(log_probabilities, targets)\n",
    "        # compute loss\n",
    "        loss = - torch.sum(targets * log_probabilities)\n",
    "        return loss\n",
    "\n",
    "    # implement Jacobian\n",
    "    @staticmethod\n",
    "    def backward(ctx, result):\n",
    "        # get results stored from forward pass\n",
    "        log_probabilities, targets = ctx.saved_tensors\n",
    "        # compute derivative of loss w.r.t. the logits\n",
    "        dJ_dz = torch.exp(log_probabilities) - targets\n",
    "        # return the derivatives; none for derivative for the targets\n",
    "        return dJ_dz, None"
   ],
   "id": "c64362e3378c29f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Image manipulation\n",
    "(should not be part of the exam)"
   ],
   "id": "b515eebdfc9f5231"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Fast Gradient Sign (FGS)",
   "id": "f731b28c58c99ab7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def FGS(x, t, network, loss, alpha=0.3):\n",
    "    # tell autograd that we need the gradient for the input\n",
    "    x.requires_grad_(True)\n",
    "    # forward input\n",
    "    z = network(x)\n",
    "    # compute loss and gradient\n",
    "    J = loss(z, t)\n",
    "    J.backward()\n",
    "    # get the gradient\n",
    "    gradient = x.grad\n",
    "    # create FGS adversarial sample\n",
    "    adversarial_sample = x + alpha * torch.sign(gradient)\n",
    "    adversarial_sample = torch.clamp(adversarial_sample, min=0, max=1)\n",
    "\n",
    "    return adversarial_sample"
   ],
   "id": "c73ae1593efd89a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Fast Gradient Value",
   "id": "c160baf9cb6dae19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def FGV(x, t, network, loss, alpha=0.6):\n",
    "    # tell autograd that we need the gradient for the input\n",
    "    x.requires_grad_(True)\n",
    "    # forward input\n",
    "    z = network(x)\n",
    "    # compute loss and gradient\n",
    "    J = loss(z, t)\n",
    "    J.backward()\n",
    "    # get the gradient\n",
    "    gradient = x.grad\n",
    "    max_abs_gradient = gradient.abs().view(gradient.shape[0], -1).max(dim=1)[0].view(-1, 1, 1, 1)\n",
    "\n",
    "    adversarial_sample = x + alpha * gradient / max_abs_gradient\n",
    "    \n",
    "    # create FGV adversarial sample\n",
    "    # adversarial_sample = X + alpha * gradient\n",
    "    adversarial_sample = torch.clamp(adversarial_sample, min=0, max=1)\n",
    "\n",
    "    return adversarial_sample"
   ],
   "id": "ed227a70c4d29ca8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Noise",
   "id": "d3bbabec3df877d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def noise(x, alpha=0.3):\n",
    "    x = x.to(device)\n",
    "    # generate noise\n",
    "    noise = torch.randint(0, 2, x.shape).float().to(device) * 2 - 1\n",
    "    # Add noise and clamp\n",
    "    noisy_sample = torch.clamp(x + alpha * noise, min=0, max=1)\n",
    "\n",
    "    return noisy_sample"
   ],
   "id": "8d3791bfaf45067f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training Loops\n",
    "\n",
    "**IMPORTANT:** *do not forget to set the network to training mode `network.train()` and then to `network.eval()`"
   ],
   "id": "517ebe782996d16b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Basic\n",
    "With Stochastic Gradient Optimizer"
   ],
   "id": "8bcb3a6c342e34d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def basic_train(network, X_train, T_train, X_val, T_val, loss_function, learning_rate=0.1, epochs=10000):\n",
    "    optimizer = torch.optim.SGD(params=network.parameters(), lr=learning_rate)\n",
    "    # collect loss and accuracy values\n",
    "    train_loss, train_acc, val_loss, val_acc = [], [], [], []\n",
    "    for epoch in range(epochs):\n",
    "        # train on training set\n",
    "        optimizer.zero_grad()\n",
    "        # ... compute network output on training data\n",
    "        Z = network(X_train)\n",
    "        # ... compute loss from network output and target data\n",
    "        loss = loss_function(Z, T_train)\n",
    "        # ... perform parameter update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # ... remember loss\n",
    "        train_loss.append(loss.item())\n",
    "        # ... compute training set accuracy\n",
    "        train_acc.append(accuracy(Z, T_train).item())\n",
    "\n",
    "        # test on validation data\n",
    "        with torch.no_grad():\n",
    "            # ... compute network output on validation data\n",
    "            Z_v = network(X_val)\n",
    "            # ... compute loss from network output and target data\n",
    "            loss_v = loss_function(Z_v, T_val)\n",
    "            # ... remember loss\n",
    "            val_loss.append(loss_v.item())\n",
    "            # ... compute validation set accuracy\n",
    "            val_acc.append(accuracy(Z_v, T_val).item())\n",
    "\n",
    "    # return the four lists of losses and accuracies\n",
    "    return train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "\n",
    "# call basic train\n",
    "results = basic_train(network=simple_fc_network, X_train=X_train, T_train=T_train, X_val=X_val, T_val=T_val,\n",
    "                      loss_function=loss)"
   ],
   "id": "5c8ea07e484f4f24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### With batch loss",
   "id": "c6efb8c03049655a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def batch_train(network, epochs, eta, momentum):\n",
    "    # select loss function and optimizer\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(network.parameters(), lr=eta, momentum=momentum)\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    network = network.to(device)\n",
    "    # collect loss values and accuracies over the training epochs\n",
    "    val_loss, val_acc = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch \", epoch)\n",
    "        # train network on training data\n",
    "        for x, t in trainloader:\n",
    "            # put data to device\n",
    "            z = network(x.to(device))\n",
    "            # train\n",
    "            optimizer.zero_grad()\n",
    "            J = loss(z, t.to(device))\n",
    "            J.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # test network on test data\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            for x, t in testloader:\n",
    "                # put data to device\n",
    "                x = x.to(device)\n",
    "                t = t.to(device)\n",
    "                # compute validation loss\n",
    "                z = network(x)\n",
    "                J = loss(z, t)\n",
    "                # compute validation accuracy\n",
    "                correct += torch.sum(torch.argmax(z, dim=1) == t).item()\n",
    "                total_loss += J.item() * len(t)\n",
    "            acc = correct / len(testset)\n",
    "            avg_loss = total_loss / len(testset)\n",
    "            val_loss.append(avg_loss)\n",
    "            val_acc.append(acc)\n",
    "\n",
    "    # return loss and accuracy values\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "# call batch train\n",
    "fc_loss, fc_acc = batch_train(network=triple_fc_network, epochs=100, eta=0.01, momentum=0.9)\n",
    "cv_loss, cv_acc = batch_train(network=double_conv_network, epochs=100, eta=0.01, momentum=0.9)"
   ],
   "id": "c176d9004a9e6c3c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Train with evaluation",
   "id": "83460a5fcd257ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_eval(network, epochs=5, lr=0.001, momentum=0.9):\n",
    "    device = torch.device(\"mps\")\n",
    "    network.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(network.parameters(), lr=lr, momentum=momentum)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    total_train_loss, total_train_accuracy = 0, 0\n",
    "    total_samples = 0\n",
    "\n",
    "    train_loss, train_acc, val_loss, val_acc = 0, 0, 0, 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training process\n",
    "        network.train()\n",
    "        batch_train_loss, batch_train_accuracy = [], []\n",
    "        for x, t in trainloader:\n",
    "            x, t = x.to(device), t.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            J = loss(network(x), t)\n",
    "            J.backward()\n",
    "            optimizer.step()\n",
    "            batch_train_loss.append(J.item() * x.size(0))\n",
    "            batch_train_accuracy.append((network(x).argmax(dim=1) == 1).float().mean().item() * x.size(0))\n",
    "            total_samples += x.size(0)\n",
    "\n",
    "            train_loss = sum(batch_train_loss) / total_samples\n",
    "            train_acc += sum(batch_train_accuracy) / total_samples\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}:\")\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "        # testing process\n",
    "        network.eval()\n",
    "        total_val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            batch_val_loss, batch_val_accuracy = [], []\n",
    "            for x, t in testloader:\n",
    "                x, t = x.to(device), t.to(device)\n",
    "                J = loss(network(x), t)\n",
    "                batch_val_loss.append(J.item() * x.size(0))\n",
    "                batch_val_accuracy.append((network(x).argmax(dim=1) == t).float().mean().item() * x.size(0))\n",
    "                total_val_samples += x.size(0)\n",
    "\n",
    "        val_loss = sum(batch_val_loss) / total_val_samples\n",
    "        val_acc = sum(batch_val_accuracy) / total_val_samples\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # Save predictions and target labels of the test set after the last epoch\n",
    "    pred, target = [], []  #Store only the test results\n",
    "    with torch.no_grad():\n",
    "        for x, t in testloader:\n",
    "            x, t = x.to(device), t.to(device)\n",
    "            pred.append(network(x).argmax(dim=1).cpu().numpy())\n",
    "            target.append(t.cpu().numpy())\n",
    "\n",
    "    return pred, target\n",
    "\n",
    "\n",
    "pred_unfrozen, targ_unfrozen = train_eval(network=network_replaced_last_layer)"
   ],
   "id": "8039fda013615cc0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training with confidence",
   "id": "f20ed4b28d6e0741"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train(network, epochs, eta, momentum, loss_function, V1=True):\n",
    "    # Set GPU\n",
    "    network = network.to(device)\n",
    "    # SGD optimizer with appropriate learning rate\n",
    "    optimizer = torch.optim.SGD(network.parameters(), lr=eta, momentum=momentum)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # evaluate average confidence for training and validation set\n",
    "        train_conf = validation_conf = 0.\n",
    "        network.train()\n",
    "        for x, t in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            # extract logits (and deep features) from network\n",
    "            logits, deep_features = network(x.to(device))\n",
    "            # compute loss\n",
    "            if V1:\n",
    "                loss = loss_function.apply(logits, t.to(device))\n",
    "            else:\n",
    "                loss = loss_function(logits, t.to(device))\n",
    "            # perform weight update\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # compute training confidence\n",
    "            train_conf += confidence(logits, t.to(device))\n",
    "\n",
    "        network.eval()\n",
    "        # compute validation confidence\n",
    "        with torch.no_grad():\n",
    "            for x, t in testloader:\n",
    "                # extract logits (and deep features)\n",
    "                logits, deep_features = network(x.to(device))\n",
    "                # compute validation confidence\n",
    "                validation_conf += confidence(logits, t.to(device))\n",
    "        # print average confidence for training and validation\n",
    "        print(\n",
    "            f\"\\rEpoch {epoch}; train: {train_conf / len(trainset):1.5f}, val: {validation_conf / len(validationset):1.5f}\")\n",
    "\n",
    "    return network"
   ],
   "id": "f0a0839a88f68264"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Simple train\n",
    "Optimizer is a parameter passed to the function"
   ],
   "id": "313077a48117fcb8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def simple_train(network, train_dataloader, optimizer, loss, device, epochs=50):\n",
    "    network.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        total_sample = 0\n",
    "        network.train()\n",
    "        for x, t in train_dataloader:\n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y = network(x)\n",
    "            J = loss(y, t)\n",
    "            J.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += J.item() * x.size(0)\n",
    "            total_sample += x.size(0)\n",
    "\n",
    "        # print average loss for training and validation\n",
    "        print(f\"\\rEpoch {epoch + 1}; train loss: {train_loss / total_sample:1.5f}\")\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "epochs = 50\n",
    "optimizer_gail = torch.optim.Adam(lstm_gail_network.parameters(), lr=0.05)\n",
    "train(lstm_gail_network, gail_train_dataloader, optimizer_gail, loss, device, epochs)"
   ],
   "id": "6edf0c22cf666f6c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Plotting",
   "id": "3ec618e959471fba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Line chart with two variables",
   "id": "12ded47c2f3ac990"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_line_chart_two_variables_with_range(X, Theta, R):\n",
    "    # create list from mult dimensional array\n",
    "    x_data = np.array([record[0] for record in X])\n",
    "    t_data = np.array([record[1] for record in X])\n",
    "    # first, plot data samples -> style 'x' as point\n",
    "    pyplot.plot(x_data[:, 1], t_data, \"rx\", label=\"Data\")\n",
    "    # define equidistant points from min (R[0]) to max (R[1]) to evaluate the network\n",
    "    x = np.arange(R[0], R[1], 100)\n",
    "    # compute the network outputs for these values\n",
    "    y = [simple_network(np.array([1, x_]), Theta)[0] for x_ in x]\n",
    "    # plot network approximation -> as a line\n",
    "    pyplot.plot(x, y, \"k-\", label=\"network\")\n",
    "    pyplot.legend()\n",
    "\n",
    "\n",
    "pyplot.subplot(131)\n",
    "plot_line_chart_two_variables_with_range(X1, None, [-1.5, 1.5])\n",
    "pyplot.title('Dataset X1')"
   ],
   "id": "d32757b3bece721c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Train vs validation loss",
   "id": "54bcabbe6b63211d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_train_vs_val_loss(train_loss, train_acc, val_loss, val_acc):\n",
    "    pyplot.figure(figsize=(10, 3))\n",
    "    ax = pyplot.subplot(121)\n",
    "    ax.plot(train_loss, \"g-\", label=\"Training set loss\")\n",
    "    ax.plot(val_loss, \"b-\", label=\"Validation set loss\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax = pyplot.subplot(122)\n",
    "    ax.plot(train_acc, \"g-\", label=\"Training set accuracy\")\n",
    "    ax.plot(val_acc, \"b-\", label=\"Validation set accuracy\")\n",
    "    ax.legend()"
   ],
   "id": "f07edc7d6a73c524"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### MNIST images plot",
   "id": "ae91c3b1630c0769"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pyplot.rcParams['image.cmap'] = 'gray'\n",
    "fig, axes = pyplot.subplots(4, 10, figsize=(10, 4))\n",
    "\n",
    "# 4 x 10 images\n",
    "index = 0\n",
    "for i in range(4):\n",
    "    for j in range(10):\n",
    "        img, _ = trainset[index]\n",
    "        axes[i][j].imshow(img, cmap='gray')\n",
    "        axes[i][j].axis(\"off\")\n",
    "        index += 1\n"
   ],
   "id": "f3bf3619972ee620"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Loss plots",
   "id": "ce5550bc4110588d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pyplot.figure(figsize=(10, 3))\n",
    "ax = pyplot.subplot(121)\n",
    "# plot loss values of FC and CV network over epochs\n",
    "ax.plot(fc_loss, \"g-\", label=\"Fully connected loss\")\n",
    "ax.plot(cv_loss, \"b-\", label=\"Convolutional loss\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Validation Loss\")\n",
    "\n",
    "ax = pyplot.subplot(122)\n",
    "# plot accuracy values of FC and CV network over epochs\n",
    "ax.plot(fc_acc, \"g-\", label=\"Fully connected accuracy\")\n",
    "ax.plot(cv_acc, \"b-\", label=\"Convolutional accuracy\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Validation Accuracy\")"
   ],
   "id": "754031415e79066f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Confusion Matrix",
   "id": "fe09a3190ae6b860"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "classes = trainset.classes\n",
    "\n",
    "pred_unfrozen_flat = np.concatenate(pred_unfrozen)\n",
    "targ_unfrozen_flat = np.concatenate(targ_unfrozen)\n",
    "\n",
    "# compute confusion matrix\n",
    "matrix_unfrozen = confusion_matrix(pred_unfrozen_flat,\n",
    "                                   targ_unfrozen_flat)  # Use predictions and target from the fine-tuned network without frozen layers\n",
    "\n",
    "# plot confusion matrices\n",
    "plot_conf_matrix = ConfusionMatrixDisplay(matrix_unfrozen, display_labels=classes)\n",
    "plot_conf_matrix.plot(xticks_rotation=\"vertical\")\n",
    "plt.show()"
   ],
   "id": "b434c8c50f905645"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Feature Magnitude Plot",
   "id": "963e525005106ff5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_features(network):\n",
    "    # collect feature magnitudes for\n",
    "    known, negative, unknown = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # extract deep features magnitudes for validation set\n",
    "        for x, t in validationloader:\n",
    "            # extract deep features (and logits)\n",
    "            logits, deep_features = network(x)\n",
    "            # compute norms\n",
    "            norms = torch.norm(deep_features, dim=1)\n",
    "            # split between known and unknown\n",
    "            batch_known, targets_known, batch_unknown = split_known_unknown(norms, t)\n",
    "            # collect norms of known samples\n",
    "            known.extend(batch_known)\n",
    "            # collect norms of negative samples\n",
    "            negative.extend(batch_unknown)\n",
    "\n",
    "        for x, t in testloader:\n",
    "            # extract deep features (and logits)\n",
    "            logits, deep_features = network(x)\n",
    "            # compute norms\n",
    "            norms = torch.norm(deep_features, dim=1)\n",
    "            # split between known and unknown\n",
    "            batch_known, targets_known, batch_unknown = split_known_unknown(norms, t)\n",
    "            # collect norms of known samples\n",
    "            known.extend(batch_known)\n",
    "            # collect norms of unknown samples\n",
    "            unknown.extend(batch_unknown)\n",
    "\n",
    "    # plot the norms as histograms\n",
    "    pyplot.figure(figsize=(4, 2))\n",
    "\n",
    "    # keep the same maximum magnitude\n",
    "    max_mag = 20\n",
    "    # plot the three histograms\n",
    "    pyplot.hist(known, bins=100, range=(0, max_mag), density=True, color=\"g\", histtype=\"step\", label=\"Known\")\n",
    "    pyplot.hist(negative, bins=100, range=(0, max_mag), density=True, color=\"b\", histtype=\"step\", label=\"Negative\")\n",
    "    pyplot.hist(unknown, bins=100, range=(0, max_mag), density=True, color=\"r\", histtype=\"step\", label=\"Unknown\")\n",
    "\n",
    "    # beautify plot\n",
    "    pyplot.legend()\n",
    "    pyplot.xlabel(\"Deep Feature Magnitude\")\n",
    "    pyplot.ylabel(\"Density\")\n",
    "\n",
    "\n",
    "plot_features(module_network_adapted)"
   ],
   "id": "6e19715c7f34e5d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Stock Plots",
   "id": "d85786907339a6e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "stock_data = {\n",
    "    'GAIL': (gail_data[0], train_gail_scaled, test_gail_scaled),  # gail_data[0] contains dates for GAIL\n",
    "   #  'NTPC': (ntpc_data[0], train_ntpc_scaled, test_ntpc_scaled)  # ntpc_data[0] contains dates for NTPC\n",
    "}\n",
    "\n",
    "# Create a figure with two subplots side by side (1 row, 2 columns)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "for idx, (stock_name, (dates, train_data, test_data)) in enumerate(stock_data.items()):\n",
    "    plt.subplot(1, 2, idx + 1)\n",
    "    # Plot the training data on the left side\n",
    "    plt.plot(dates[:len(train_data)], train_data, label='Training')\n",
    "    # Plot the test data on the right side\n",
    "    plt.plot(dates[len(train_data):], test_data, label='Test')\n",
    "    # Add a vertical line at 2018-01-01 for reference\n",
    "    date = mdates.date2num(np.datetime64('2018-01-01'))\n",
    "    plt.axvline(date, color='black', ls='--')\n",
    "    # Set labels and title for the subplot\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title(stock_name)\n",
    "    # Display legend for the plotted lines\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', linewidth=0.5, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "b5d6ead721cc527e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Theory",
   "id": "9d2008a6de109049"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Compute gradient\n",
    " -> See cheatsheet - [derivatives](Derivative%20Rules%20Cheatsheet.md)"
   ],
   "id": "2e3b64133dd4ba30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Convolutional Networks\n",
   "id": "fc719a3ca2ba4761"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Network output calculations\n",
    "\n",
    "***Output size of network -> (in + 2p - k) / s + 1***\n",
    "\n",
    "1. A [2D convolutional layer](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) with $Q_1$ channels, kernel size $7\\times7$, stride 1 and padding 0.\n",
    "2. A [2D maximum pooling](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) with pooling size $2\\times2$ and stride 2.\n",
    "3. A `Sigmoid` activation function.\n",
    "4. A 2D convolutional layer with $Q_2$ channels, kernel size $5\\times5$, stride 1 and padding 2.\n",
    "5. A 2D maximum pooling with pooling size $2\\times2$ and stride 2.\n",
    "6. A `Sigmoid` activation function.\n",
    "7. A flattening layer to turn the 3D feature map into a 1D vector.\n",
    "8. A fully-connected layer with the appropriate number of inputs and $O$ outputs.\n",
    "\n",
    "Consider the network as defined above\n",
    "Assume that the input is a $28\\times28$ grayscale image.\n",
    "How many hidden neurons do we need in the final fully-connected layer for a given number $Q_2$ of output channels of the second convolution?\n",
    "\n",
    "(Write steps of computation.)\n",
    "\n",
    "1. Input image size: $28\\times28$\n",
    "2. 1st Convolutional layer: $(28 + 2*0 - 7) / 1 + 1 = 22\\times22$\n",
    "3. 1st Max pooling layer: $(22/2)\\times(22/2) = 11\\times11$\n",
    "4. 1st Sigmoid activation function: no influence\n",
    "5. 2nd Convolutional layer: $(11 + 2*2 - 5) / 1 + 1 = 11\\times11$\n",
    "6. 2nd Max pooling layer: $(11/2)\\times(11/2) = 5.5\\times5.5$ -> round up: $6\\times6$\n",
    "7. 2nd Sigmoid activation function: no influence\n",
    "8. Flattening layer: $6\\times6 = 36$"
   ],
   "id": "a0588b59f7f6972"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Learnable parameters calculations\n",
    "\n",
    "1. A `torch.nn.Flatten` layer to turn the $28\\times28$ pixel image (2D) into a $28*28$ pixel vector (1D)\n",
    "2. A fully-connected layer with D input neurons and K1 outputs.\n",
    "3. A `Sigmoid` activation function.\n",
    " 4. A fully-connected layer with K1 input neurons and K2 outputs.\n",
    "5. A `Sigmoid`activation function.\n",
    "6. A fully-connected layer with K2 input neurons and O outputs.\n",
    "\n",
    "fully_connected(D=28*28, K1=128, K2=64, O=10)\n",
    "\n",
    "> #### Fully-connected Network:\n",
    ">\n",
    "> - first fully-connected layer: $(28*28+1)*128 =$ **100480**\n",
    "> - second fully-connected layer: $(128+1)*64 =$ **8256**\n",
    "> - third fully-connected layer: $(64+1)*10 =$ **650**\n",
    "> - total: $100480 + 8256 + 650 =$ **109386**\n",
    "\n",
    "1. A [2D convolutional layer](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) with $Q_1$ channels, kernel size $7\\times7$, stride 1 and padding 0.\n",
    "2. A [2D maximum pooling](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) with pooling size $2\\times2$ and stride 2.\n",
    "3. A `Sigmoid` activation function.\n",
    "4. A 2D convolutional layer with $Q_2$ channels, kernel size $5\\times5$, stride 1 and padding 2.\n",
    "5. A 2D maximum pooling with pooling size $2\\times2$ and stride 2.\n",
    "6. A `Sigmoid` activation function.\n",
    "7. A flattening layer to turn the 3D feature map into a 1D vector.\n",
    "8. A fully-connected layer with the appropriate number of inputs and $O$ outputs.\n",
    " \n",
    "convolutional(Q1=16, Q2=16, O=10)\n",
    "\n",
    "> #### Convolutional Network:\n",
    "> - first convolutional layer: $(7*7+1)*16$ = **800**\n",
    "> - second convolutional layer: $(16*5*5+1)*16$ = **6416**\n",
    "> - fully-connected layer: $(16*5*5+1)*10$ = **4010**\n",
    "> - total: $800 + 6416 + 4010 = **11226**\n",
    "\n",
    "***can check with torch.Tensor.numel()***"
   ],
   "id": "b39b784e92b47f73"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
